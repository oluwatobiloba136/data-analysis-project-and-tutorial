<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-09-03">
  <title>Introduction to Gaussian Processes</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
$$\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
$$
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Introduction to Gaussian Processes</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-09-03</time></p>
  <p class="venue" style="text-align:center">Gaussian Process Summer School, Sheffield</p>
</section>

<section class="slide level3">

<!-- To compile -->
<p><!--% not ipynb--></p>
<p><!--% not notes--></p>
</section>
<section id="section" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/gp/rasmussen-williams-book.jpg" width="50%" align="" style="background:none; border:none; box-shadow:none;"></p>
<p><span align="right"><span class="citation" data-cites="Rasmussen:book06">Rasmussen and Williams (2006)</span></span></p>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</div>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li><p>Gold medal times for Olympic Marathon since 1896.</p></li>
<li><p>Marathons before 1924 didn’t have a standardised distance.</p></li>
<li><p>Present results using pace per km.</p></li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<img src="../slides/diagrams/Stephen_Kiprotich.jpg" alt="image" /> <small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg">
</object>
</section>
<section id="overdetermined-system" class="slide level3">
<h3>Overdetermined System</h3>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<script>
showDivs(1, 'over_determined_system');
</script>
<button onclick="plusDivs(-1, 'over_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'over_determined_system')">
❯
</button>
<p><input id="range-over_determined_system" type="range" min="1" max="8" value="1" onchange="setDivs('over_determined_system')" oninput="setDivs('over_determined_system')"> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system001.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system002.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system003.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system004.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system005.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system006.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system007.svg"></object></p>
</section>
<section id="datascalar-minputscalar-c" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span></p>
<p><span class="math display">\[2.5 = 2m + c\]</span></p>
</div>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<img class="" src="../slides/diagrams/ml/Pierre-Simon_Laplace.png" width="30%" align="center" style="background:none; border:none; box-shadow:none;"> {
<center>
<em>Pierre Simon Laplace </em>
</center>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<iframe frameborder="0" scrolling="no" style="border:0px" src="http://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PR17-IA2&amp;output=embed" , width="700" height="500">
</iframe>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/laplacesDeterminismEnglish.png" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<iframe frameborder="0" scrolling="no" style="border:0px" src="http://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PR17-IA4&amp;output=embed" , width="700" height="500">
</iframe>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/philosophicaless00lapliala.png" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="datascalar-minputscalar-c-noisescalar" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c + \noiseScalar\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c + \noiseScalar_1
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c + \noiseScalar_2
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span> <span class="math display">\[
2.5 = 2m + c + \noiseScalar_3
\]</span></p>
</div>
</section>
<section id="a-probabilistic-process" class="slide level3">
<h3>A Probabilistic Process</h3>
<div class="fragment">
<p>Set the mean of Gaussian to be a function. <span class="math display">\[p
\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i-\mappingFunction\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
</div>
</section>
<section id="the-gaussian-density" class="slide level3">
<h3>The Gaussian Density</h3>
<ul>
<li>Perhaps the most common probability density.</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\begin{align}
  p(\dataScalar| \meanScalar, \dataStd^2) &amp; = \frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{(\dataScalar - \meanScalar)^2}{2\dataStd^2}\right)\\&amp; \buildrel\triangle\over = \gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2}
  \end{align}\]</span></p>
</div>
</section>
<section id="gaussian-density" class="slide level3">
<h3>Gaussian Density</h3>
<object class="svgplot " align data="../slides/diagrams/ml/gaussian_of_height.svg">
</object>
<center>
<em>The Gaussian PDF with <span class="math inline">\({\meanScalar}=1.7\)</span> and variance <span class="math inline">\({\dataStd}^2=0.0225\)</span>. Mean shown as cyan line. It could represent the heights of a population of students. </em>
</center>
</section>
<section id="gaussian-density-1" class="slide level3">
<h3>Gaussian Density</h3>
<p><large><span class="math display">\[
\gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2} = \frac{1}{\sqrt{2\pi\dataStd^2}} \exp\left(-\frac{(\dataScalar-\meanScalar)^2}{2\dataStd^2}\right)
\]</span></large></p>
<div class="fragment">
<center>
<span class="math inline">\(\dataStd^2\)</span> is the variance of the density and <span class="math inline">\(\meanScalar\)</span> is the mean.
</center>
</div>
</section>
<section id="two-important-gaussian-properties" class="slide level3">
<h3>Two Important Gaussian Properties</h3>
</section>
<section id="sum-of-gaussians" class="slide level3">
<h3>Sum of Gaussians</h3>
<div class="fragment">
<p><span align="left">Sum of Gaussian variables is also Gaussian.</span></p>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the sum is distributed as</span></p>
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData \sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><small>(<em>Aside</em>: As sum increases, sum of non-Gaussian, finite variance variables is also Gaussian because of <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>.)</small></p>
</div>
</section>
<section id="scaling-a-gaussian" class="slide level3">
<h3>Scaling a Gaussian</h3>
<div class="fragment">
<p><span align="left">Scaling a Gaussian leads to a Gaussian.</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\sigma^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the scaled variable is distributed as</span></p>
<p><span class="math display">\[\mappingScalar \dataScalar \sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \sigma^2}.\]</span></p>
</div>
</section>
<section id="regression-examples" class="slide level3">
<h3>Regression Examples</h3>
<ul>
<li>Predict a real value, <span class="math inline">\(\dataScalar_i\)</span> given some inputs <span class="math inline">\(\inputVector_i\)</span>.</li>
<li>Predict quality of meat given spectral measurements (Tecator data).</li>
<li>Radiocarbon dating, the C14 calibration curve: predict age given quantity of C14 isotope.</li>
<li>Predict quality of different Go or Backgammon moves given expert rated training data.</li>
</ul>
<!-- SECTION Underdetermined System -->
</section>
<section id="underdetermined-system" class="slide level3">
<h3>Underdetermined System</h3>
<ul>
<li>What about two unknowns and <em>one</em> observation? <span class="math display">\[\dataScalar_1 =  m\inputScalar_1 + c\]</span></li>
</ul>
<div class="fragment">
<p>Can compute <span class="math inline">\(m\)</span> given <span class="math inline">\(c\)</span>. <span class="math display">\[m = \frac{\dataScalar_1 - c}{\inputScalar}\]</span></p>
</div>
</section>
<section id="underdetermined-system-1" class="slide level3">
<h3>Underdetermined System</h3>

<script>
showDivs(1, 'under_determined_system');
</script>
<button onclick="plusDivs(-1, 'under_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'under_determined_system')">
❯
</button>
<p><input id="range-under_determined_system" type="range" min="1" max="3" value="1" onchange="setDivs('under_determined_system')" oninput="setDivs('under_determined_system')"> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system000.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system001.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system002.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system003.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system004.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system005.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system006.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system007.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system008.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system009.svg"></object></p>
</section>
<section id="overdetermined-system-1" class="slide level3">
<h3>Overdetermined System</h3>
<ul>
<li><p>With two unknowns and two observations: <span class="math display">\[\begin{aligned}
      \dataScalar_1 = &amp; m\inputScalar_1 + c\\
      \dataScalar_2 = &amp; m\inputScalar_2 + c
    \end{aligned}\]</span></p></li>
<li><p>Additional observation leads to <em>overdetermined</em> system. <span class="math display">\[\dataScalar_3 =  m\inputScalar_3 + c\]</span></p></li>
</ul>
</section>
<section id="overdetermined-system-2" class="slide level3">
<h3>Overdetermined System</h3>
<ul>
<li>This problem is solved through a noise model <span class="math inline">\(\noiseScalar \sim \gaussianSamp{0}{\dataStd^2}\)</span> <span class="math display">\[\begin{aligned}
      \dataScalar_1 = m\inputScalar_1 + c + \noiseScalar_1\\
      \dataScalar_2 = m\inputScalar_2 + c + \noiseScalar_2\\
      \dataScalar_3 = m\inputScalar_3 + c + \noiseScalar_3
    \end{aligned}\]</span></li>
</ul>
</section>
<section id="noise-models" class="slide level3">
<h3>Noise Models</h3>
<ul>
<li>We aren’t modeling entire system.</li>
<li>Noise model gives mismatch between model and data.</li>
<li>Gaussian model justified by appeal to central limit theorem.</li>
<li>Other models also possible (Student-<span class="math inline">\(t\)</span> for heavy tails).</li>
<li>Maximum likelihood with Gaussian noise leads to <em>least squares</em>.</li>
</ul>
</section>
<section id="probability-for-under--and-overdetermined" class="slide level3">
<h3>Probability for Under- and Overdetermined</h3>
<ul>
<li>To deal with overdetermined introduced probability distribution for ‘variable’, <span class="math inline">\({\noiseScalar}_i\)</span>.</li>
</ul>
<div class="fragment">
<ul>
<li>For underdetermined system introduced probability distribution for ‘parameter’, <span class="math inline">\(c\)</span>.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>This is known as a Bayesian treatment.</li>
</ul>
</div>
</section>
<section id="different-types-of-uncertainty" class="slide level3">
<h3>Different Types of Uncertainty</h3>
<ul>
<li>The first type of uncertainty we are assuming is <em>aleatoric</em> uncertainty.</li>
<li>The second type of uncertainty we are assuming is <em>epistemic</em> uncertainty.</li>
</ul>
</section>
<section id="aleatoric-uncertainty" class="slide level3">
<h3>Aleatoric Uncertainty</h3>
<ul>
<li>This is uncertainty we couldn’t know even if we wanted to. e.g. the result of a football match before it’s played.</li>
<li>Where a sheet of paper might land on the floor.</li>
</ul>
</section>
<section id="epistemic-uncertainty" class="slide level3">
<h3>Epistemic Uncertainty</h3>
<ul>
<li>This is uncertainty we could in principle know the answer too. We just haven’t observed enough yet, e.g. the result of a football match <em>after</em> it’s played.</li>
<li>What colour socks your lecturer is wearing.</li>
</ul>
</section>
<section id="bayesian-regression" class="slide level3">
<h3>Bayesian Regression</h3>
</section>
<section id="prior-distribution" class="slide level3">
<h3>Prior Distribution</h3>
<ul>
<li>Bayesian inference requires a prior on the parameters.</li>
<li>The prior represents your belief <em>before</em> you see the data of the likely value of the parameters.</li>
<li>For linear regression, consider a Gaussian prior on the intercept:</li>
</ul>
<p><span class="math display">\[c \sim \gaussianSamp{0}{\alpha_1}\]</span></p>
</section>
<section id="posterior-distribution" class="slide level3">
<h3>Posterior Distribution</h3>
<ul>
<li>Posterior distribution is found by combining the prior with the likelihood.</li>
<li>Posterior distribution is your belief <em>after</em> you see the data of the likely value of the parameters.</li>
<li>The posterior is found through <strong>Bayes’ Rule</strong> <span class="math display">\[
  p(c|\dataScalar) = \frac{p(\dataScalar|c)p(c)}{p(\dataScalar)}
  \]</span></li>
</ul>
<p><span class="math display">\[
  \text{posterior} = \frac{\text{likelihood}\times \text{prior}}{\text{marginal likelihood}}.
  \]</span></p>
</section>
<section id="bayes-update" class="slide level3">
<h3>Bayes Update</h3>

<script>
showDivs(1, 'dem_gaussian');
</script>
<button onclick="plusDivs(-1, 'dem_gaussian')">
❮
</button>
<button onclick="plusDivs(1, 'dem_gaussian')">
❯
</button>
<p><input id="range-dem_gaussian" type="range" min="1" max="3" value="1" onchange="setDivs('dem_gaussian')" oninput="setDivs('dem_gaussian')"> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian001.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian002.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian003.svg"></object></p>
</section>
<section id="stages-to-derivation-of-the-posterior" class="slide level3">
<h3>Stages to Derivation of the Posterior</h3>
<ul>
<li>Multiply likelihood by prior</li>
<li>they are “exponentiated quadratics”, the answer is always also an exponentiated quadratic because <span class="math inline">\(\exp(a^2)\exp(b^2) = \exp(a^2 + b^2)\)</span>.</li>
<li>Complete the square to get the resulting density in the form of a Gaussian.</li>
<li>Recognise the mean and (co)variance of the Gaussian. This is the estimate of the posterior.</li>
</ul>
</section>
<section id="multivariate-system" class="slide level3">
<h3>Multivariate System</h3>
<ul>
<li>For general Bayesian inference need multivariate priors.</li>
</ul>
<div class="fragment">
<ul>
<li>E.g. for multivariate linear regression:</li>
</ul>
<p><span class="math display">\[\dataScalar_i = \sum_j \weightScalar_j \inputScalar_{i, j} + \noiseScalar_i,\]</span></p>
<p><span class="math display">\[\dataScalar_i = \weightVector^\top \inputVector_{i, :} + \noiseScalar_i.\]</span></p>
<p>(where we’ve dropped <span class="math inline">\(c\)</span> for convenience), we need a prior over <span class="math inline">\(\weightVector\)</span>.</p>
</div>
</section>
<section id="multivariate-system-1" class="slide level3">
<h3>Multivariate System</h3>
<ul>
<li>This motivates a <em>multivariate</em> Gaussian density.</li>
</ul>
<div class="fragment">
<ul>
<li>We will use the multivariate Gaussian to put a prior <em>directly</em> on the function (a Gaussian process).</li>
</ul>
</div>
</section>
<section id="multivariate-bayesian-regression" class="slide level3">
<h3>Multivariate Bayesian Regression</h3>
</section>
<section id="multivariate-regression-likelihood" class="slide level3">
<h3>Multivariate Regression Likelihood</h3>
<ul>
<li>Noise corrupted data point <span class="math display">\[\dataScalar_i = \weightVector^\top \inputVector_{i, :} + {\noiseScalar}_i\]</span></li>
</ul>
<div class="fragment">
<ul>
<li>Multivariate regression likelihood: <span class="math display">\[p(\dataVector| \inputMatrix, \weightVector) = \frac{1}{\left(2\pi {\dataStd}^2\right)^{\numData/2}} \exp\left(-\frac{1}{2{\dataStd}^2}\sum_{i=1}^{\numData}\left(\dataScalar_i - \weightVector^\top \inputVector_{i, :}\right)^2\right)\]</span></li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Now use a <em>multivariate</em> Gaussian prior: <span class="math display">\[p(\weightVector) = \frac{1}{\left(2\pi \alpha\right)^\frac{\dataDim}{2}} \exp \left(-\frac{1}{2\alpha} \weightVector^\top \weightVector\right)\]</span></li>
</ul>
</div>
</section>
<section id="two-dimensional-gaussian-distribution" class="slide level3">
<h3>Two Dimensional Gaussian Distribution</h3>
</section>
<section id="two-dimensional-gaussian" class="slide level3">
<h3>Two Dimensional Gaussian</h3>
<ul>
<li>Consider height, <span class="math inline">\(h/m\)</span> and weight, <span class="math inline">\(w/kg\)</span>.</li>
<li>Could sample height from a distribution: <span class="math display">\[
  p(h) \sim \gaussianSamp{1.7}{0.0225}.
  \]</span></li>
<li>And similarly weight: <span class="math display">\[
  p(w) \sim \gaussianSamp{75}{36}.
  \]</span></li>
</ul>
</section>
<section id="height-and-weight-models" class="slide level3">
<h3>Height and Weight Models</h3>
<object class="svgplot " align data="../slides/diagrams/ml/height_weight_gaussian.svg">
</object>
<center>
<em>Gaussian distributions for height and weight. </em>
</center>
</section>
<section id="independence-assumption" class="slide level3">
<h3>Independence Assumption</h3>
<ul>
<li>We assume height and weight are independent.</li>
</ul>
<p><span class="math display">\[
  p(w, h) = p(w)p(h).
  \]</span></p>
</section>
<section id="sampling-two-dimensional-variables" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>

<script>
showDivs(0, 'independent_height_weight');
</script>
<button onclick="plusDivs(-1, 'independent_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'independent_height_weight')">
❯
</button>
<p><input id="range-independent_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('independent_height_weight')" oninput="setDivs('independent_height_weight')"> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight000.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight001.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight002.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight003.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight004.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight005.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight006.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight007.svg"></object></p>
</section>
<section id="body-mass-index" class="slide level3">
<h3>Body Mass Index</h3>
<ul>
<li>In reality they are dependent (body mass index) <span class="math inline">\(= \frac{w}{h^2}\)</span>.</li>
<li>To deal with this dependence we introduce <em>correlated</em> multivariate Gaussians.</li>
</ul>
</section>
<section id="sampling-two-dimensional-variables-1" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>

<script>
showDivs(0, 'correlated_height_weight');
</script>
<button onclick="plusDivs(-1, 'correlated_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'correlated_height_weight')">
❯
</button>
<p><input id="range-correlated_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('correlated_height_weight')" oninput="setDivs('correlated_height_weight')"> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight000.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight001.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight002.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight003.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight004.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight005.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight006.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight007.svg"></object></p>
</section>
<section id="independent-gaussians" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = p(w)p(h)
\]</span></p>
</section>
<section id="independent-gaussians-1" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi \dataStd_1^2}\sqrt{2\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\frac{(w-\meanScalar_1)^2}{\dataStd_1^2} + \frac{(h-\meanScalar_2)^2}{\dataStd_2^2}\right)\right)
\]</span></p>
</section>
<section id="independent-gaussians-2" class="slide level3">
<h3>Independent Gaussians</h3>
<p><small> <span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi\dataStd_1^22\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)^\top\begin{bmatrix}\dataStd_1^2&amp; 0\\0&amp;\dataStd_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)\right)
\]</span> </small></p>
</section>
<section id="independent-gaussians-3" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-1" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)^\top\mathbf{D}^{-1}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-2" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\rotationMatrix\mathbf{D}^{-1}\rotationMatrix^\top(\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix^{-1} = \rotationMatrix \mathbf{D}^{-1} \rotationMatrix^\top
\]</span></p>
</section>
<section id="correlated-gaussian-3" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\covarianceMatrix}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\covarianceMatrix^{-1} (\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix = \rotationMatrix \mathbf{D} \rotationMatrix^\top
\]</span></p>
</section>
<section id="multivariate-gaussian-properties" class="slide level3">
<h3>Multivariate Gaussian Properties</h3>
</section>
<section id="recall-univariate-gaussian-properties" class="slide level3">
<h3>Recall Univariate Gaussian Properties</h3>
<div class="fragment">
<ol type="1">
<li>Sum of Gaussian variables is also Gaussian.</li>
</ol>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\dataStd_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData\dataStd_i^2}\]</span></p>
</div>
</section>
<section id="recall-univariate-gaussian-properties-1" class="slide level3">
<h3>Recall Univariate Gaussian Properties</h3>
<ol start="2" type="1">
<li>Scaling a Gaussian leads to a Gaussian.</li>
</ol>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\dataStd^2}\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\mappingScalar\dataScalar\sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \dataStd^2}\]</span></p>
</div>
</section>
<section id="multivariate-consequence" class="slide level3">
<h3>Multivariate Consequence</h3>
<p><span align="left">If</span> <span class="math display">\[\inputVector \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\]</span></p>
<div class="fragment">
<p><span align="left">And</span> <span class="math display">\[\dataVector= \mappingMatrix\inputVector\]</span></p>
</div>
<div class="fragment">
<p><span align="left">Then</span> <span class="math display">\[\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top}\]</span></p>
</div>
</section>
<section id="linear-gaussian-models" class="slide level3">
<h3>Linear Gaussian Models</h3>
<p>Gaussian processes are initially of interest because 1. linear Gaussian models are easier to deal with 2. Even the parameters <em>within</em> the process can be handled, by considering a particular limit.</p>
</section>
<section id="multivariate-gaussian-properties-1" class="slide level3">
<h3>Multivariate Gaussian Properties</h3>
<ul>
<li><p>If <span class="math display">\[
\dataVector = \mappingMatrix \inputVector + \noiseVector,
\]</span></p></li>
<li><p>Assume <span class="math display">\[\begin{align}
\inputVector &amp; \sim \gaussianSamp{\meanVector}{\covarianceMatrix}\\
\noiseVector &amp; \sim \gaussianSamp{\zerosVector}{\covarianceMatrixTwo}
\end{align}\]</span></p></li>
<li><p>Then <span class="math display">\[
\dataVector \sim \gaussianSamp{\mappingMatrix\meanVector}{\mappingMatrix\covarianceMatrix\mappingMatrix^\top + \covarianceMatrixTwo}.
\]</span> If <span class="math inline">\(\covarianceMatrixTwo=\dataStd^2\eye\)</span>, this is Probabilistic Principal Component Analysis <span class="citation" data-cites="Tipping:probpca99">(Tipping and Bishop, 1999)</span>, because we integrated out the inputs (or <em>latent</em> variables they would be called in that case).</p></li>
</ul>
</section>
<section id="non-linear-on-inputs" class="slide level3">
<h3>Non linear on Inputs</h3>
<ul>
<li>Set each activation function computed at each data point to be</li>
</ul>
<p><span class="math display">\[
\activationScalar_{i,j} = \activationScalar(\mappingVector^{(1)}_{j}, \inputVector_{i})
\]</span> Define <em>design matrix</em> <span class="math display">\[
\activationMatrix = 
\begin{bmatrix}
\activationScalar_{1, 1} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numHidden} \\
\activationScalar_{1, 2} &amp; \activationScalar_{1, 2} &amp; \dots &amp; \activationScalar_{1, \numData} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\activationScalar_{\numData, 1} &amp; \activationScalar_{\numData, 2} &amp; \dots &amp; \activationScalar_{\numData, \numHidden}
\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-representation-of-a-neural-network" class="slide level3">
<h3>Matrix Representation of a Neural Network</h3>
<p><span class="math display">\[\dataScalar\left(\inputVector\right) = \activationVector\left(\inputVector\right)^\top \mappingVector + \noiseScalar\]</span></p>
<div class="fragment">
<p><span class="math display">\[\dataVector = \activationMatrix\mappingVector + \noiseVector\]</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2\eye}\]</span></p>
</div>
</section>
<section id="prior-density" class="slide level3">
<h3>Prior Density</h3>
<ul>
<li>Define</li>
</ul>
<p><span class="math display">\[
\mappingVector \sim \gaussianSamp{\zerosVector}{\alpha\eye},
\]</span></p>
<ul>
<li>Rules of multivariate Gaussians to see that,</li>
</ul>
<p><span class="math display">\[
\dataVector \sim \gaussianSamp{\zerosVector}{\alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye}.
\]</span></p>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="joint-gaussian-density" class="slide level3">
<h3>Joint Gaussian Density</h3>
<ul>
<li>Elements are a function <span class="math inline">\(\kernel_{i,j} = \kernel\left(\inputVector_i, \inputVector_j\right)\)</span></li>
</ul>
<p><span class="math display">\[
\kernelMatrix = \alpha \activationMatrix \activationMatrix^\top + \dataStd^2 \eye.
\]</span></p>
</section>
<section id="covariance-function" class="slide level3">
<h3>Covariance Function</h3>
<p><span class="math display">\[
\kernel_\mappingFunction\left(\inputVector_i, \inputVector_j\right) = \alpha \activationVector\left(\mappingMatrix_1, \inputVector_i\right)^\top \activationVector\left(\mappingMatrix_1, \inputVector_j\right)
\]</span></p>
<ul>
<li>formed by inner products of the rows of the <em>design matrix</em>.</li>
</ul>
</section>
<section id="gaussian-process" class="slide level3">
<h3>Gaussian Process</h3>
<ul>
<li><p>Instead of making assumptions about our density over each data point, <span class="math inline">\(\dataScalar_i\)</span> as i.i.d.</p></li>
<li><p>make a joint Gaussian assumption over our data.</p></li>
<li><p>covariance matrix is now a function of both the parameters of the activation function, <span class="math inline">\(\mappingMatrix_1\)</span>, and the input variables, <span class="math inline">\(\inputMatrix\)</span>.</p></li>
<li><p>Arises from integrating out <span class="math inline">\(\mappingVector^{(2)}\)</span>.</p></li>
</ul>
</section>
<section id="basis-functions" class="slide level3">
<h3>Basis Functions</h3>
<ul>
<li>Can be very complex, such as deep kernels, <span class="citation" data-cites="Cho:deep09">(Cho and Saul, 2009)</span> or could even put a convolutional neural network inside.</li>
<li>Viewing a neural network in this way is also what allows us to beform sensible <em>batch</em> normalizations <span class="citation" data-cites="Ioffe:batch15">(Ioffe and Szegedy, 2015)</span>.</li>
</ul>
</section>
<section id="distributions-over-functions" class="slide level3">
<h3>Distributions over Functions</h3>
<p><a href="file:///Users/neil/lawrennd/talks_gp/includes/gp-intro-very-short.md">_gp/includes/gp-intro-very-short.md</a></p>

</section>
<section id="section-7" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample001.svg">
</object>
</section>
<section id="section-8" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample002.svg">
</object>
</section>
<section id="section-9" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample003.svg">
</object>
</section>
<section id="section-10" class="slide level3" data-transition="none">
<h3></h3>
<object class="svgplot " align data="../slides/diagrams/gp/gp_rejection_sample004.svg">
</object>
</section>
<section id="section-11" class="slide level3" data-transition="none">
<h3></h3>
<p>x <object class="svgplot " align="" data="../slides/diagrams/gp/gp_rejection_sample005.svg"></object></p>
<p><a href="file:///Users/neil/lawrennd/talks_gp/includes/gpdistfunc.md">_gp/includes/gpdistfunc.md</a></p>
</section>
<section id="sampling-a-function" class="slide level3">
<h3>Sampling a Function</h3>
<p><strong>Multi-variate Gaussians</strong></p>
<ul>
<li>We will consider a Gaussian with a particular structure of covariance matrix.</li>
<li>Generate a single sample from this 25 dimensional Gaussian density, <span class="math display">\[
\mappingFunctionVector=\left[\mappingFunction_{1},\mappingFunction_{2}\dots \mappingFunction_{25}\right].
\]</span></li>
<li>We will plot these points against their index.</li>
</ul>

</section>
<section id="gaussian-distribution-sample" class="slide level3">
<h3>Gaussian Distribution Sample</h3>
<script>
showDivs(0, 'two_point_sample');
</script>
<button onclick="plusDivs(-1, 'two_point_sample')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample')">
❯
</button>
<p><input id="range-two_point_sample" type="range" min="0" max="8" value="0" onchange="setDivs('two_point_sample')" oninput="setDivs('two_point_sample')"> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample000.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample001.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample002.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample003.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample004.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample005.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample006.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample007.svg"></object> <object class="svgplot two_point_sample" align="" data="../slides/diagrams/gp/two_point_sample008.svg"></object></p>
<p><a href="file:///Users/neil/lawrennd/talks_gp/includes/gaussian-predict-index-one-and-two.md">_gp/includes/gaussian-predict-index-one-and-two.md</a></p>

</section>
<section id="prediction-of-mappingfunction_2-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{2}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(9, 'two_point_sample2');
</script>
<button onclick="plusDivs(-1, 'two_point_sample2')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample2')">
❯
</button>
<p><input id="range-two_point_sample2" type="range" min="9" max="12" value="9" onchange="setDivs('two_point_sample2')" oninput="setDivs('two_point_sample2')"> <object class="svgplot two_point_sample2" align="" data="../slides/diagrams/gp/two_point_sample009.svg"></object> <object class="svgplot two_point_sample2" align="" data="../slides/diagrams/gp/two_point_sample010.svg"></object> <object class="svgplot two_point_sample2" align="" data="../slides/diagrams/gp/two_point_sample011.svg"></object> <object class="svgplot two_point_sample2" align="" data="../slides/diagrams/gp/two_point_sample012.svg"></object></p>
</section>
<section id="uluru" class="slide level3">
<h3>Uluru</h3>
<p><img class="" src="../slides/diagrams/gp/799px-Uluru_Panorama.jpg" width="" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="prediction-with-correlated-gaussians" class="slide level3">
<h3>Prediction with Correlated Gaussians</h3>
<ul>
<li>Prediction of <span class="math inline">\(\mappingFunction_2\)</span> from <span class="math inline">\(\mappingFunction_1\)</span> requires <em>conditional density</em>.</li>
<li>Conditional density is <em>also</em> Gaussian. <span class="math display">\[
p(\mappingFunction_2|\mappingFunction_1) = \gaussianDist{\mappingFunction_2}{\frac{\kernelScalar_{1, 2}}{\kernelScalar_{1, 1}}\mappingFunction_1}{ \kernelScalar_{2, 2} - \frac{\kernelScalar_{1,2}^2}{\kernelScalar_{1,1}}}
\]</span> where covariance of joint density is given by <span class="math display">\[
\kernelMatrix = \begin{bmatrix} \kernelScalar_{1, 1} &amp; \kernelScalar_{1, 2}\\ \kernelScalar_{2, 1} &amp; \kernelScalar_{2, 2}.\end{bmatrix}
\]</span></li>
</ul>
<p><a href="file:///Users/neil/lawrennd/talks_gp/includes/gaussian-predict-index-one-and-eight.md">_gp/includes/gaussian-predict-index-one-and-eight.md</a></p>

</section>
<section id="prediction-of-mappingfunction_8-from-mappingfunction_1" class="slide level3">
<h3>Prediction of <span class="math inline">\(\mappingFunction_{8}\)</span> from <span class="math inline">\(\mappingFunction_{1}\)</span></h3>
<script>
showDivs(13, 'two_point_sample3');
</script>
<button onclick="plusDivs(-1, 'two_point_sample3')">
❮
</button>
<button onclick="plusDivs(1, 'two_point_sample3')">
❯
</button>
<p><input id="range-two_point_sample3" type="range" min="13" max="17" value="13" onchange="setDivs('two_point_sample3')" oninput="setDivs('two_point_sample3')"></p>
<object class="svgplot two_point_sample3" align data="../slides/diagrams/gp/two_point_sample013.svg">
</object>
<object class="svgplot two_point_sample3" align data="../slides/diagrams/gp/two_point_sample014.svg">
</object>
<object class="svgplot two_point_sample3" align data="../slides/diagrams/gp/two_point_sample015.svg">
</object>
<object class="svgplot two_point_sample3" align data="../slides/diagrams/gp/two_point_sample016.svg">
</object>
<object class="svgplot two_point_sample3" align data="../slides/diagrams/gp/two_point_sample017.svg">
</object>
<p><a href="file:///Users/neil/lawrennd/talks_kern/includes/computing-rbf-covariance.md">_kern/includes/computing-rbf-covariance.md</a></p>
</section>
<section id="where-did-this-covariance-matrix-come-from" class="slide level3">
<h3>Where Did This Covariance Matrix Come From?</h3>
<p><span class="math display">\[
k(\inputVector, \inputVector^\prime) = \alpha \exp\left(-\frac{\left\Vert \inputVector - \inputVector^\prime\right\Vert^2_2}{2\lengthScale^2}\right)\]</span></p>
<table>
<tr>
<td width="50%">
<ul>
<li><p>Covariance matrix is built using the <em>inputs</em> to the function, <span class="math inline">\(\inputVector\)</span>.</p></li>
<li><p>For the example above it was based on Euclidean distance.</p></li>
<li>The covariance function is also know as a kernel.</li>
</ul>
</td>
<td width="50%">
<object class="svgplot " align data="../slides/diagrams/kern/eq_covariance.svg">
</object>
</td>
</tr>
</table>
</section>
<section id="computing-covariance" class="slide level3">
<h3>Computing Covariance</h3>

<script>
showDivs(0, 'computing_eq_three_covariance');
</script>
<button onclick="plusDivs(-1, 'computing_eq_three_covariance')">
❮
</button>
<button onclick="plusDivs(1, 'computing_eq_three_covariance')">
❯
</button>
<p><input id="range-computing_eq_three_covariance" type="range" min="0" max="16" value="0" onchange="setDivs('computing_eq_three_covariance')" oninput="setDivs('computing_eq_three_covariance')"> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance000.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance001.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance002.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance003.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance004.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance005.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance006.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance007.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance008.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance009.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance010.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance011.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance012.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance013.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance014.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance015.svg"></object> <object class="svgplot computing_eq_three_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_covariance016.svg"></object></p>
</section>
<section id="computing-covariance-1" class="slide level3">
<h3>Computing Covariance</h3>

<script>
showDivs(0, 'computing_eq_four_covariance');
</script>
<button onclick="plusDivs(-1, 'computing_eq_four_covariance')">
❮
</button>
<button onclick="plusDivs(1, 'computing_eq_four_covariance')">
❯
</button>
<p><input id="range-computing_eq_four_covariance" type="range" min="0" max="27" value="0" onchange="setDivs('computing_eq_four_covariance')" oninput="setDivs('computing_eq_four_covariance')"> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance000.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance001.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance002.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance003.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance004.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance005.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance006.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance007.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance008.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance009.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance010.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance011.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance012.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance013.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance014.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance015.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance016.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance017.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance018.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance019.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance020.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance021.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance022.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance023.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance024.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance025.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance026.svg"></object> <object class="svgplot computing_eq_four_covariance" align="" data="../slides/diagrams/kern/computing_eq_four_covariance027.svg"></object></p>
</section>
<section id="computing-covariance-2" class="slide level3">
<h3>Computing Covariance</h3>

<script>
showDivs(0, 'computing_eq_three_2_covariance');
</script>
<button onclick="plusDivs(-1, 'computing_eq_three_2_covariance')">
❮
</button>
<button onclick="plusDivs(1, 'computing_eq_three_2_covariance')">
❯
</button>
<p><input id="range-computing_eq_three_2_covariance" type="range" min="0" max="16" value="0" onchange="setDivs('computing_eq_three_2_covariance')" oninput="setDivs('computing_eq_three_2_covariance')"> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance000.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance001.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance002.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance003.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance004.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance005.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance006.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance007.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance008.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance009.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance010.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance011.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance012.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance013.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance014.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance015.svg"></object> <object class="svgplot computing_eq_three_2_covariance" align="" data="../slides/diagrams/kern/computing_eq_three_2_covariance016.svg"></object></p>
</section>
<section id="polynomial-covariance" class="slide level3">
<h3>Polynomial Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha(w \inputVector^\top\inputVector^\prime + b)^d\]</span>
</center>
<br>
<table>
<tr>
<td width="45%">
<object class align data="../slides/diagrams/kern/polynomial_covariance.svg">
</object>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/polynomial_covariance.gif" width="100%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="brownian-covariance" class="slide level3">
<h3>Brownian Covariance</h3>
<p><span class="math display">\[
\kernelScalar(t, t^\prime) = \alpha \min(t, t^\prime)
\]</span></p>
<!--<table><tr><td width="50%">
<object class="svgplot " align="" data="../slides/diagrams/kern/brownian_covariance.svg"></object>
</td><td width="50%">
<iframe src="../slides/diagrams/kern/brownian_covariance.html" width="512" height="384" allowtransparency="true" frameborder="0">
</iframe>
</td></tr></table>
-->
<table>
<tr>
<td width="45%">
<object class align data="../slides/diagrams/kern/brownian_covariance.svg">
</object>
</td>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/brownian_covariance.gif" width="100%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="periodic-covariance" class="slide level3">
<h3>Periodic Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha\exp\left(\frac{-2\sin(\pi rw)^2}{\lengthScale^2}\right)\]</span>
</center>
<br>
<table>
<tr>
<td width="45%">
<object class align data="../slides/diagrams/kern/periodic_covariance.svg">
</object>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/periodic_covariance.gif" width="100%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
</section>
<section id="covariance-functions" class="slide level3">
<h3>Covariance Functions</h3>
<p><strong>RBF Basis Functions</strong></p>
<p><span class="math display">\[
\basisFunction_k(\inputScalar) = \exp\left(-\frac{\ltwoNorm{\inputScalar-\meanScalar_k}^{2}}{\lengthScale^{2}}\right).
\]</span></p>
<p><span class="math display">\[
\meanVector = \begin{bmatrix} -1 \\ 0 \\ 1\end{bmatrix},
\]</span></p>
<p><span class="math display">\[
\kernelScalar\left(\inputVals,\inputVals^{\prime}\right)=\alpha\basisVector(\inputVals)^\top \basisVector(\inputVals^\prime).
\]</span></p>
</section>
<section id="basis-function-covariance" class="slide level3">
<h3>Basis Function Covariance</h3>
<center>
<span class="math display">\[\kernel(\inputVector, \inputVector^\prime) = \basisVector(\inputVector)^\top \basisVector(\inputVector^\prime)\]</span>
</center>
<br>
<table>
<tr>
<td width="45%">
<object class align data="../slides/diagrams/kern/basis_covariance.svg">
</object>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/basis_covariance.gif" width="100%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
{
<center>
<em>A covariance function based on a non-linear basis given by <span class="math inline">\(\basisVector(\inputVector)\)</span>. </em>
</center>
</section>
<section id="selecting-number-and-location-of-basis" class="slide level3">
<h3>Selecting Number and Location of Basis</h3>
<ul>
<li>Need to choose</li>
</ul>
<ol type="1">
<li>location of centers</li>
<li>number of basis functions Restrict analysis to 1-D input, <span class="math inline">\(\inputScalar\)</span>.</li>
</ol>
<ul>
<li>Consider uniform spacing over a region:</li>
</ul>
</section>
<section id="uniform-basis-functions" class="slide level3">
<h3>Uniform Basis Functions</h3>
<ul>
<li><p>Set each center location to <span class="math display">\[\locationScalar_k = a+\Delta\locationScalar\cdot (k-1).\]</span></p></li>
<li><p>Specify the basis functions in terms of their indices, <span class="math display">\[\begin{aligned}
\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = &amp;\alpha^\prime\Delta\locationScalar \sum_{k=1}^{\numBasisFunc} \exp\Bigg(
  -\frac{\inputScalar_i^2 + \inputScalar_j^2}{2\rbfWidth^2}\\ 
   &amp; - \frac{2\left(a+\Delta\locationScalar\cdot (k-1)\right)
   \left(\inputScalar_i+\inputScalar_j\right) + 2\left(a+\Delta\locationScalar \cdot (k-1)\right)^2}{2\rbfWidth^2} \Bigg)
  \end{aligned}\]</span></p></li>
</ul>
<p>where we’ve scaled variance of process by <span class="math inline">\(\Delta\locationScalar\)</span>.</p>
</section>
<section id="infinite-basis-functions" class="slide level3">
<h3>Infinite Basis Functions</h3>
<ul>
<li><p>Take <span class="math display">\[
  \locationScalar_1=a \ \text{and}\  \locationScalar_\numBasisFunc=b \ \text{so}\ b= a+ \Delta\locationScalar\cdot(\numBasisFunc-1)
  \]</span></p></li>
<li><p>This implies <span class="math display">\[
  b-a = \Delta\locationScalar (\numBasisFunc -1)
  \]</span> and therefore <span class="math display">\[
  \numBasisFunc = \frac{b-a}{\Delta \locationScalar} + 1
  \]</span></p></li>
<li><p>Take limit as <span class="math inline">\(\Delta\locationScalar\rightarrow 0\)</span> so <span class="math inline">\(\numBasisFunc \rightarrow \infty\)</span> where we have used <span class="math inline">\(a + k\cdot\Delta\locationScalar\rightarrow \locationScalar\)</span>.</p></li>
</ul>
</section>
<section id="result" class="slide level3">
<h3>Result</h3>
<ul>
<li>Performing the integration leads to <span class="math display">\[\begin{aligned}
\kernelScalar(\inputScalar_i,&amp;\inputScalar_j) = \alpha^\prime \sqrt{\pi\rbfWidth^2}
\exp\left( -\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right)\\ &amp;\times
\frac{1}{2}\left[\text{erf}\left(\frac{\left(b - \frac{1}{2}\left(\inputScalar_i +
\inputScalar_j\right)\right)}{\rbfWidth} \right)-
\text{erf}\left(\frac{\left(a - \frac{1}{2}\left(\inputScalar_i +
   \inputScalar_j\right)\right)}{\rbfWidth} \right)\right],
\end{aligned}\]</span></li>
<li>Now take limit as <span class="math inline">\(a\rightarrow -\infty\)</span> and <span class="math inline">\(b\rightarrow \infty\)</span> <span class="math display">\[\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = \alpha\exp\left(
-\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right).\]</span> where <span class="math inline">\(\alpha=\alpha^\prime \sqrt{\pi\rbfWidth^2}\)</span>.</li>
</ul>
</section>
<section id="infinite-feature-space" class="slide level3">
<h3>Infinite Feature Space</h3>
<ul>
<li>An RBF model with infinite basis functions is a Gaussian process.</li>
<li>The covariance function is given by the  covariance function. <span class="math display">\[\kernelScalar\left(\inputScalar_i,\inputScalar_j\right) = \alpha \exp\left(
      -\frac{\left(\inputScalar_i-\inputScalar_j\right)^2}{4\rbfWidth^2}\right).\]</span></li>
</ul>
</section>
<section id="infinite-feature-space-1" class="slide level3">
<h3>Infinite Feature Space</h3>
<ul>
<li>An RBF model with infinite basis functions is a Gaussian process.</li>
<li>The covariance function is the exponentiated quadratic (squared exponential).</li>
<li><strong>Note:</strong> The functional form for the covariance function and basis functions are similar.</li>
<li>this is a special case,</li>
<li>in general they are very different</li>
</ul>
</section>
<section id="mlp-covariance" class="slide level3">
<h3>MLP Covariance</h3>
<center>
<span class="math display">\[\kernelScalar(\inputVector, \inputVector^\prime) = \alpha \arcsin\left(\frac{w \inputVector^\top \inputVector^\prime + b}{\sqrt{\left(w \inputVector^\top \inputVector + b + 1\right)\left(w \left.\inputVector^\prime\right.^\top \inputVector^\prime + b + 1\right)}}\right)\]</span>
</center>
<br>
<table>
<tr>
<td width="45%">
<object class align data="../slides/diagrams/kern/mlp_covariance.svg">
</object>
<td width="45%">
<img class="negate" src="../slides/diagrams/kern/mlp_covariance.gif" width="100%" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
<!--include{_kern/includes/relu-covariance.md}-->
</section>
<section id="sinc-covariance" class="slide level3">
<h3>Sinc Covariance</h3>

</section>
<section id="references" class="slide level3">
<h3>References</h3>


<div id="refs" class="references">
<div id="ref-Cho:deep09">
<p>Cho, Y., Saul, L.K., 2009. Kernel methods for deep learning, in: Bengio, Y., Schuurmans, D., Lafferty, J.D., Williams, C.K.I., Culotta, A. (Eds.), Advances in Neural Information Processing Systems 22. Curran Associates, Inc., pp. 342–350.</p>
</div>
<div id="ref-Ioffe:batch15">
<p>Ioffe, S., Szegedy, C., 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift, in: Bach, F., Blei, D. (Eds.), Proceedings of the 32nd International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, Lille, France, pp. 448–456.</p>
</div>
<div id="ref-Rasmussen:book06">
<p>Rasmussen, C.E., Williams, C.K.I., 2006. Gaussian processes for machine learning. mit, Cambridge, MA.</p>
</div>
<div id="ref-Tipping:probpca99">
<p>Tipping, M.E., Bishop, C.M., 1999. Probabilistic principal component analysis. Journal of the Royal Statistical Society, B 6, 611–622. <a href="https://doi.org/doi:10.1111/1467-9868.00196" class="uri">https://doi.org/doi:10.1111/1467-9868.00196</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
