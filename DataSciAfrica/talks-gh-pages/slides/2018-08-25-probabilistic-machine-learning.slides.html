<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-08-25">
  <title>Probabilistic Machine Learning</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
$$\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
$$
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Probabilistic Machine Learning</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-08-25</time></p>
  <p class="venue" style="text-align:center">AI Saturdays, Lagos</p>
</section>

<section class="slide level3">

<p><!--% not ipynb--></p>
<p><!--% not notes--></p>
<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
<!-- SECTION Probabilities -->
<!-- SECTION Movie Body Count Example -->
</div>
</section>
<section id="pods" class="slide level3">
<h3><code>pods</code></h3>
<p>The <code>pods</code> library is a library for supporting open data science (python open data science). It allows you to load in various data sets and provides tools for helping teach in the notebook.</p>
<p>To install pods you can use pip:</p>
<p><code>pip install pods</code></p>
<p>The code is also available on github: <a href="https://github.com/sods/ods" class="uri">https://github.com/sods/ods</a></p>
<p>Once <code>pods</code> is installed, it can be imported in the usual manner.</p>
<!-- SECTION Conditioning -->
</section>
<section id="probability-review" class="slide level3">
<h3>Probability Review</h3>
<ul>
<li>We are interested in trials which result in two random variables, <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, each of which has an ‘outcome’ denoted by <span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>.</li>
<li>We summarise the notation and terminology for these distributions in the following table.</li>
</ul>
</section>
<section id="section" class="slide level3">
<h3></h3>
<table>
<thead>
<tr class="header">
<th>Terminology</th>
<th>Mathematical notation</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>joint</td>
<td><span class="math inline">\(P(X=x, Y=y)\)</span></td>
<td>prob. that X=x <em>and</em> Y=y</td>
</tr>
<tr class="even">
<td>marginal</td>
<td><span class="math inline">\(P(X=x)\)</span></td>
<td>prob. that X=x <em>regardless of</em> Y</td>
</tr>
<tr class="odd">
<td>conditional</td>
<td><span class="math inline">\(P(X=x\vert Y=y)\)</span></td>
<td>prob. that X=x <em>given that</em> Y=y</td>
</tr>
</tbody>
</table>
<center>
The different basic probability distributions.
</center>
</section>
<section id="a-pictorial-definition-of-probability" class="slide level3">
<h3>A Pictorial Definition of Probability</h3>
<object class="svgplot " align data="../slides/diagrams/mlai/prob_diagram.svg">
</object>
<p><span align="right">Inspired by lectures from Christopher Bishop</span></p>
</section>
<section id="definition-of-probability-distributions." class="slide level3">
<h3>Definition of probability distributions.</h3>
<table>
<colgroup>
<col style="width: 20%" />
<col style="width: 46%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="header">
<th>Terminology</th>
<th>Definition</th>
<th>Probability Notation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Joint Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=3,Y=4}}{N}\)</span></td>
<td><span class="math inline">\(P\left(X=3,Y=4\right)\)</span></td>
</tr>
<tr class="even">
<td>Marginal Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=5}}{N}\)</span></td>
<td><span class="math inline">\(P\left(X=5\right)\)</span></td>
</tr>
<tr class="odd">
<td>Conditional Probability</td>
<td><span class="math inline">\(\lim_{N\rightarrow\infty}\frac{n_{X=3,Y=4}}{n_{Y=4}}\)</span></td>
<td><span class="math inline">\(P\left(X=3\vert Y=4\right)\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="notational-details" class="slide level3">
<h3>Notational Details</h3>
<ul>
<li><p>Typically we should write out <span class="math inline">\(P\left(X=x,Y=y\right)\)</span>.</p></li>
<li>In practice, we often use <span class="math inline">\(P\left(x,y\right)\)</span>.</li>
<li>This looks very much like we might write a multivariate function, <em>e.g.</em> <span class="math inline">\(f\left(x,y\right)=\frac{x}{y}\)</span>.</li>
<li>For a multivariate function though, <span class="math inline">\(f\left(x,y\right)\neq f\left(y,x\right)\)</span>.</li>
<li>However <span class="math inline">\(P\left(x,y\right)=P\left(y,x\right)\)</span> because <span class="math inline">\(P\left(X=x,Y=y\right)=P\left(Y=y,X=x\right)\)</span>.</li>
<li><p>We now quickly review the ‘rules of probability’.</p></li>
</ul>
</section>
<section id="normalization" class="slide level3">
<h3>Normalization</h3>
<p><em>All</em> distributions are normalized. This is clear from the fact that <span class="math inline">\(\sum_{x}n_{x}=N\)</span>, which gives <span class="math display">\[\sum_{x}P\left(x\right)={\lim_{N\rightarrow\infty}}\frac{\sum_{x}n_{x}}{N}={\lim_{N\rightarrow\infty}}\frac{N}{N}=1.\]</span> A similar result can be derived for the marginal and conditional distributions.</p>
</section>
<section id="the-product-rule" class="slide level3">
<h3>The Product Rule</h3>
<ul>
<li><span class="math inline">\(P\left(x|y\right)\)</span> is <span class="math display">\[
  {\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{n_{y}}.
  \]</span></li>
<li><span class="math inline">\(P\left(x,y\right)\)</span> is <span class="math display">\[
  {\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{N}={\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{n_{y}}\frac{n_{y}}{N}
  \]</span> or in other words <span class="math display">\[
  P\left(x,y\right)=P\left(x|y\right)P\left(y\right).
  \]</span> This is known as the product rule of probability.</li>
</ul>
</section>
<section id="the-sum-rule" class="slide level3">
<h3>The Sum Rule</h3>
<p>Ignoring the limit in our definitions: * The marginal probability <span class="math inline">\(P\left(y\right)\)</span> is <span class="math inline">\({\lim_{N\rightarrow\infty}}\frac{n_{y}}{N}\)</span> . * The joint distribution <span class="math inline">\(P\left(x,y\right)\)</span> is <span class="math inline">\({\lim_{N\rightarrow\infty}}\frac{n_{x,y}}{N}\)</span>. * <span class="math inline">\(n_{y}=\sum_{x}n_{x,y}\)</span> so <span class="math display">\[
  {\lim_{N\rightarrow\infty}}\frac{n_{y}}{N}={\lim_{N\rightarrow\infty}}\sum_{x}\frac{n_{x,y}}{N},
  \]</span> in other words <span class="math display">\[
  P\left(y\right)=\sum_{x}P\left(x,y\right).
  \]</span> This is known as the sum rule of probability.</p>
</section>
<section id="bayes-rule" class="slide level3">
<h3>Bayes’ Rule</h3>
<ul>
<li>From the product rule, <span class="math display">\[
  P\left(y,x\right)=P\left(x,y\right)=P\left(x|y\right)P\left(y\right),\]</span> so <span class="math display">\[
  P\left(y|x\right)P\left(x\right)=P\left(x|y\right)P\left(y\right)
  \]</span> which leads to Bayes’ rule, <span class="math display">\[
  P\left(y|x\right)=\frac{P\left(x|y\right)P\left(y\right)}{P\left(x\right)}.
  \]</span></li>
</ul>
</section>
<section id="bayes-theorem-example" class="slide level3">
<h3>Bayes’ Theorem Example</h3>
<ul>
<li>There are two barrels in front of you. Barrel One contains 20 apples and 4 oranges. Barrel Two other contains 4 apples and 8 oranges. You choose a barrel randomly and select a fruit. It is an apple. What is the probability that the barrel was Barrel One?</li>
</ul>
</section>
<section id="bayes-theorem-example-answer-i" class="slide level3">
<h3>Bayes’ Theorem Example: Answer I</h3>
<ul>
<li>We are given that: <span class="math display">\[\begin{aligned}
P(\text{F}=\text{A}|\text{B}=1) = &amp; 20/24 \\
P(\text{F}=\text{A}|\text{B}=2) = &amp; 4/12 \\
P(\text{B}=1) = &amp; 0.5 \\
P(\text{B}=2) = &amp; 0.5
  \end{aligned}\]</span></li>
</ul>
</section>
<section id="bayes-theorem-example-answer-ii" class="slide level3">
<h3>Bayes’ Theorem Example: Answer II</h3>
<ul>
<li>We use the sum rule to compute: <span class="math display">\[\begin{aligned}
P(\text{F}=\text{A}) = &amp; P(\text{F}=\text{A}|\text{B}=1)P(\text{B}=1) \\&amp; + P(\text{F}=\text{A}|\text{B}=2)P(\text{B}=2) \\
      = &amp; 20/24\times 0.5 + 4/12 \times 0.5 = 7/12
   \end{aligned}\]</span></li>
<li>And Bayes’ theorem tells us that: <span class="math display">\[\begin{aligned}
P(\text{B}=1|\text{F}=\text{A}) = &amp; \frac{P(\text{F} = \text{A}|\text{B}=1)P(\text{B}=1)}{P(\text{F}=\text{A})}\\ 
     = &amp; \frac{20/24 \times 0.5}{7/12} = 5/7
  \end{aligned}\]</span></li>
</ul>
</section>
<section id="reading-exercises" class="slide level3">
<h3>Reading &amp; Exercises</h3>
<ul>
<li><span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> on probability distributions: page 12–17 (Section 1.2).</li>
<li>Complete Exercise 1.3 in <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span>.</li>
</ul>
</section>
<section id="computing-expectations-example" class="slide level3">
<h3>Computing Expectations Example</h3>
<ul>
<li>Consider the following distribution.</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
</tbody>
</table>
<ul>
<li>What is the mean of the distribution?</li>
<li>What is the standard deviation of the distribution?</li>
<li>Are the mean and standard deviation representative of the distribution form?</li>
<li>What is the expected value of <span class="math inline">\(-\log P(y)\)</span>?</li>
</ul>
</section>
<section id="expectations-example-answer" class="slide level3">
<h3>Expectations Example: Answer</h3>
<ul>
<li>We are given that:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2\)</span></td>
<td>1</td>
<td>4</td>
<td>9</td>
<td>16</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(-\log(P(y))\)</span></td>
<td>1.204</td>
<td>1.609</td>
<td>2.302</td>
<td>0.916</td>
</tr>
</tbody>
</table>
<ul>
<li>Mean: <span class="math inline">\(1\times 0.3 + 2\times 0.2 + 3 \times 0.1 + 4 \times 0.4 = 2.6\)</span></li>
<li>Second moment: <span class="math inline">\(1 \times 0.3 + 4 \times 0.2 + 9 \times 0.1 + 16 \times 0.4 = 8.4\)</span></li>
<li>Variance: <span class="math inline">\(8.4 - 2.6\times 2.6 = 1.64\)</span></li>
<li>Standard deviation: <span class="math inline">\(\sqrt{1.64} = 1.2806\)</span></li>
</ul>
</section>
<section id="expectations-example-answer-ii" class="slide level3">
<h3>Expectations Example: Answer II</h3>
<ul>
<li>We are given that:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(y\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(P\left(y\right)\)</span></td>
<td>0.3</td>
<td>0.2</td>
<td>0.1</td>
<td>0.4</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2\)</span></td>
<td>1</td>
<td>4</td>
<td>9</td>
<td>16</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(-\log(P(y))\)</span></td>
<td>1.204</td>
<td>1.609</td>
<td>2.302</td>
<td>0.916</td>
</tr>
</tbody>
</table>
<ul>
<li>Expectation <span class="math inline">\(-\log(P(y))\)</span>: <span class="math inline">\(0.3\times 1.204 + 0.2\times 1.609 + 0.1\times 2.302 +0.4\times 0.916 = 1.280\)</span></li>
</ul>
</section>
<section id="sample-based-approximation-example" class="slide level3">
<h3>Sample Based Approximation Example</h3>
<ul>
<li><p>You are given the following values samples of heights of students,</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
</tbody>
</table></li>
<li>What is the sample mean?</li>
<li>What is the sample variance?</li>
<li><p>Can you compute sample approximation expected value of <span class="math inline">\(-\log P(y)\)</span>?</p></li>
</ul>
</section>
<section id="sample-based-approximation-example-answer" class="slide level3">
<h3>Sample Based Approximation Example: Answer</h3>
<ul>
<li>We can compute:</li>
</ul>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
<tr class="even">
<td><span class="math inline">\(y^2_i\)</span></td>
<td>3.0976</td>
<td>2.9929</td>
<td>3.2041</td>
<td>3.2761</td>
<td>3.4225</td>
<td>3.2400</td>
</tr>
</tbody>
</table>
<ul>
<li>Mean: <span class="math inline">\(\frac{1.76 + 1.73 + 1.79 + 1.81 + 1.85 + 1.80}{6} = 1.79\)</span></li>
<li>Second moment: $  = 3.2055$</li>
<li>Variance: <span class="math inline">\(3.2055 - 1.79\times1.79 = 1.43\times 10^{-3}\)</span></li>
<li>Standard deviation: <span class="math inline">\(0.0379\)</span></li>
<li>No, you can’t compute it. You don’t have access to <span class="math inline">\(P(y)\)</span> directly.</li>
</ul>
</section>
<section id="sample-based-approximation-example-1" class="slide level3">
<h3>Sample Based Approximation Example</h3>
<ul>
<li><p>You are given the following values samples of heights of students,</p>
<table>
<thead>
<tr class="header">
<th><span class="math inline">\(i\)</span></th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(y_i\)</span></td>
<td>1.76</td>
<td>1.73</td>
<td>1.79</td>
<td>1.81</td>
<td>1.85</td>
<td>1.80</td>
</tr>
</tbody>
</table></li>
<li><p>Actually these “data” were sampled from a Gaussian with mean 1.7 and standard deviation 0.15. Are your estimates close to the real values? If not why not?</p></li>
</ul>
</section>
<section id="probabilistic-modelling" class="slide level3">
<h3>Probabilistic Modelling</h3>
<ul>
<li>Probabilistically we want, <span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*),
\]</span> <span class="math inline">\(\dataScalar_*\)</span> is a test output <span class="math inline">\(\inputVector_*\)</span> is a test input <span class="math inline">\(\inputMatrix\)</span> is a training input matrix <span class="math inline">\(\dataVector\)</span> is training outputs</li>
</ul>
</section>
<section id="joint-model-of-world" class="slide level3">
<h3>Joint Model of World</h3>
<p><span class="math display">\[
p(\dataScalar_*|\dataVector, \inputMatrix, \inputVector_*) = \int p(\dataScalar_*|\inputVector_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) \text{d} \mappingMatrix
\]</span></p>
<div class="fragment">
<p><span class="math inline">\(\mappingMatrix\)</span> contains <span class="math inline">\(\mappingMatrix_1\)</span> and <span class="math inline">\(\mappingMatrix_2\)</span></p>
<p><span class="math inline">\(p(\mappingMatrix | \dataVector, \inputMatrix)\)</span> is posterior density</p>
</div>
</section>
<section id="likelihood" class="slide level3">
<h3>Likelihood</h3>
<p><span class="math inline">\(p(\dataScalar|\inputVector, \mappingMatrix)\)</span> is the <em>likelihood</em> of data point</p>
<div class="fragment">
<p>Normally assume independence: <span class="math display">\[
p(\dataVector|\inputMatrix, \mappingMatrix) \prod_{i=1}^\numData p(\dataScalar_i|\inputVector_i, \mappingMatrix),\]</span></p>
</div>
</section>
<section id="likelihood-and-prediction-function" class="slide level3">
<h3>Likelihood and Prediction Function</h3>
<p><span class="math display">\[
p(\dataScalar_i | \mappingFunction(\inputVector_i)) = \frac{1}{\sqrt{2\pi \dataStd^2}} \exp\left(-\frac{\left(\dataScalar_i - \mappingFunction(\inputVector_i)\right)^2}{2\dataStd^2}\right)
\]</span></p>
</section>
<section id="unsupervised-learning" class="slide level3">
<h3>Unsupervised Learning</h3>
<ul>
<li><p>Can also consider priors over latents <span class="math display">\[
p(\dataVector_*|\dataVector) = \int p(\dataVector_*|\inputMatrix_*, \mappingMatrix) p(\mappingMatrix | \dataVector, \inputMatrix) p(\inputMatrix) p(\inputMatrix_*) \text{d} \mappingMatrix \text{d} \inputMatrix \text{d}\inputMatrix_*
\]</span></p></li>
<li><p>This gives <em>unsupervised learning</em>.</p></li>
</ul>
</section>
<section id="probabilistic-inference" class="slide level3">
<h3>Probabilistic Inference</h3>
<ul>
<li><p>Data: <span class="math inline">\(\dataVector\)</span></p></li>
<li><p>Model: <span class="math inline">\(p(\dataVector, \dataVector^*)\)</span></p></li>
<li><p>Prediction: <span class="math inline">\(p(\dataVector^*| \dataVector)\)</span></p></li>
</ul>
</section>
<section id="graphical-models" class="slide level3">
<h3>Graphical Models</h3>
<ul>
<li>Represent joint distribution through <em>conditional dependencies</em>.</li>
<li>E.g. Markov chain</li>
</ul>
<p><span class="math display">\[p(\dataVector) = p(\dataScalar_\numData | \dataScalar_{\numData-1}) p(\dataScalar_{\numData-1}|\dataScalar_{\numData-2}) \dots p(\dataScalar_{2} | \dataScalar_{1})\]</span></p>
<object class="svgplot " align data="../slides/diagrams/ml/markov.svg">
</object>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<p>Predict Perioperative Risk of Clostridium Difficile Infection Following Colon Surgery <span class="citation" data-cites="Steele:predictive12">(Steele et al., 2012)</span></p>
<p><img class="negate" src="../slides/diagrams/bayes-net-diagnosis.png" width="40%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="classification" class="slide level3">
<h3>Classification</h3>
<ul>
<li>We are given a data set containing ‘inputs’, <span class="math inline">\(\inputMatrix\)</span> and ‘targets’, <span class="math inline">\(\dataVector\)</span>.</li>
<li>Each data point consists of an input vector <span class="math inline">\(\inputVector_i\)</span> and a class label, <span class="math inline">\(\dataScalar_i\)</span>.</li>
<li>For binary classification assume <span class="math inline">\(\dataScalar_i\)</span> should be either <span class="math inline">\(1\)</span> (yes) or <span class="math inline">\(-1\)</span> (no).</li>
<li>Input vector can be thought of as features.</li>
</ul>
</section>
<section id="discrete-probability" class="slide level3">
<h3>Discrete Probability</h3>
<ul>
<li>Algorithms based on <em>prediction</em> function and <em>objective</em> function.</li>
<li>For regression the <em>codomain</em> of the functions, <span class="math inline">\(f(\inputMatrix)\)</span> was the real numbers or sometimes real vectors.</li>
<li>In classification we are given an input vector, <span class="math inline">\(\inputVector\)</span>, and an associated label, <span class="math inline">\(\dataScalar\)</span> which either takes the value <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>.</li>
</ul>
</section>
<section id="classification-examples" class="slide level3">
<h3>Classification Examples</h3>
<ul>
<li>Classifiying hand written digits from binary images (automatic zip code reading)</li>
<li>Detecting faces in images (e.g. digital cameras).</li>
<li>Who a detected face belongs to (e.g. Picasa, Facebook, DeepFace, GaussianFace)</li>
<li>Classifying type of cancer given gene expression data.</li>
<li>Categorization of document types (different types of news article on the internet)</li>
</ul>
</section>
<section id="reminder-on-the-term-bayesian" class="slide level3">
<h3>Reminder on the Term “Bayesian”</h3>
<ul>
<li>We use Bayes’ rule to invert probabilities in the Bayesian approach.</li>
<li>Bayesian is not named after Bayes’ rule (v. common confusion).</li>
<li>The term Bayesian refers to the treatment of the parameters as stochastic variables.</li>
<li>Proposed by <span class="citation" data-cites="Laplace:memoire74">Laplace (1774)</span> and <span class="citation" data-cites="Bayes:doctrine63">Bayes (1763)</span> independently.</li>
<li>For early statisticians this was very controversial (Fisher et al).</li>
</ul>
</section>
<section id="reminder-on-the-term-bayesian-1" class="slide level3">
<h3>Reminder on the Term “Bayesian”</h3>
<ul>
<li>The use of Bayes’ rule does <em>not</em> imply you are being Bayesian.</li>
<li>It is just an application of the product rule of probability.</li>
</ul>
</section>
<section id="bernoulli-distribution" class="slide level3">
<h3>Bernoulli Distribution</h3>
<ul>
<li>Binary classification: need a probability distribution for discrete variables.</li>
<li>Discrete probability is in some ways easier: <span class="math inline">\(P(\dataScalar=1) = \pi\)</span> &amp; specify distribution as a table.</li>
<li>Instead of <span class="math inline">\(\dataScalar=-1\)</span> for negative class we take <span class="math inline">\(\dataScalar=0\)</span>.</li>
</ul>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(\dataScalar\)</span></th>
<th style="text-align: center;">0</th>
<th style="text-align: center;">1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(P(\dataScalar)\)</span></td>
<td style="text-align: center;"><span class="math inline">\((1-\pi)\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\pi\)</span></td>
</tr>
</tbody>
</table>
<p>This is the <a href="http://en.wikipedia.org/wiki/Bernoulli_distribution">Bernoulli distribution</a>.</p>
</section>
<section id="mathematical-switch" class="slide level3">
<h3>Mathematical Switch</h3>
<ul>
<li><p>The Bernoulli distribution <span class="math display">\[
  P(\dataScalar) = \pi^\dataScalar (1-\pi)^{(1-\dataScalar)}
  \]</span></p></li>
<li><p>Is a clever trick for switching probabilities, as code it would be</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> bernoulli(y_i, pi):
    <span class="cf">if</span> y_i <span class="op">==</span> <span class="dv">1</span>:
        <span class="cf">return</span> pi
    <span class="cf">else</span>:
        <span class="cf">return</span> <span class="dv">1</span><span class="op">-</span>pi</code></pre></div>
</section>
<section id="jacob-bernoullis-bernoulli" class="slide level3">
<h3>Jacob Bernoulli’s Bernoulli</h3>
<ul>
<li>Bernoulli described the Bernoulli distribution in terms of an ‘urn’ filled with balls.</li>
<li>There are red and black balls. There is a fixed number of balls in the urn.</li>
<li>The portion of red balls is given by <span class="math inline">\(\pi\)</span>.</li>
<li>For this reason in Bernoulli’s distribution there is <em>epistemic</em> uncertainty about the distribution parameter.</li>
</ul>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<iframe frameborder="0" scrolling="no" style="border:0px" src="http://books.google.co.uk/books?id=CF4UAAAAQAAJ&amp;pg=PA87&amp;output=embed" , width="700" height="500">
</iframe>
</section>
<section id="jacob-bernoullis-bernoulli-1" class="slide level3">
<h3>Jacob Bernoulli’s Bernoulli</h3>
<object class="svgplot " align data="../slides/diagrams/ml/bernoulli-urn.svg">
</object>
</section>
<section id="thomas-bayess-bernoulli" class="slide level3">
<h3>Thomas Bayes’s Bernoulli</h3>
<ul>
<li>Bayes described the Bernoulli distribution (he didn’t call it that!) in terms of a table and two balls.</li>
<li>Each ball is rolled so it comes to rest at a uniform distribution across the table.</li>
<li>The first ball comes to rest at a position that is a <span class="math inline">\(\pi\)</span> times the width of table.</li>
<li>After placing the first ball you consider whether a second would land to the left or the right.</li>
<li>For this reason in Bayes’s distribution there is considered to be <em>aleatoric</em> uncertainty about the distribution parameter.</li>
</ul>
</section>
<section id="thomas-bayes-bernoulli" class="slide level3">
<h3>Thomas Bayes’ Bernoulli</h3>
<script>
showDivs(1, 'bayes_billiard');
</script>
<button onclick="plusDivs(-1, 'bayes_billiard')">
❮
</button>
<button onclick="plusDivs(1, 'bayes_billiard')">
❯
</button>
<p><input id="range-bayes_billiard" type="range" min="1" max="10" value="1" onchange="setDivs('bayes_billiard')" oninput="setDivs('bayes_billiard')"> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard000.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard001.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard002.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard003.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard004.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard005.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard006.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard007.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard008.svg"></object> <object class="svgplot bayes_billiard" align="" data="../slides/diagrams/ml/bayes-billiard009.svg"></object></p>
</section>
<section id="maximum-likelihood-in-the-bernoulli" class="slide level3">
<h3>Maximum Likelihood in the Bernoulli</h3>
<ul>
<li>Assume data, <span class="math inline">\(\dataVector\)</span> is binary vector length <span class="math inline">\(\numData\)</span>.</li>
<li>Assume each value was sampled independently from the Bernoulli distribution, given probability <span class="math inline">\(\pi\)</span> <span class="math display">\[
p(\dataVector|\pi) = \prod_{i=1}^{\numData} \pi^{\dataScalar_i} (1-\pi)^{1-\dataScalar_i}.
\]</span></li>
</ul>
</section>
<section id="negative-log-likelihood" class="slide level3">
<h3>Negative Log Likelihood</h3>
<ul>
<li>Minimize the negative log likelihood <span class="math display">\[\begin{align*}
  \errorFunction(\pi)&amp; = -\log p(\dataVector|\pi)\\ 
                 &amp; = -\sum_{i=1}^{\numData} \dataScalar_i \log \pi - \sum_{i=1}^{\numData} (1-\dataScalar_i) \log(1-\pi),
  \end{align*}\]</span></li>
<li>Take gradient with respect to the parameter <span class="math inline">\(\pi\)</span>. <span class="math display">\[\frac{\text{d}\errorFunction(\pi)}{\text{d}\pi} = -\frac{\sum_{i=1}^{\numData} \dataScalar_i}{\pi}  + \frac{\sum_{i=1}^{\numData} (1-\dataScalar_i)}{1-\pi},\]</span></li>
</ul>
</section>
<section id="fixed-point" class="slide level3">
<h3>Fixed Point</h3>
<ul>
<li><p>Stationary point: set derivative to zero <span class="math display">\[0 = -\frac{\sum_{i=1}^{\numData} \dataScalar_i}{\pi}  + \frac{\sum_{i=1}^{\numData} (1-\dataScalar_i)}{1-\pi},\]</span></p></li>
<li><p>Rearrange to form <span class="math display">\[(1-\pi)\sum_{i=1}^{\numData} \dataScalar_i =   \pi\sum_{i=1}^{\numData} (1-\dataScalar_i),\]</span></p></li>
<li><p>Giving <span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i =   \pi\left(\sum_{i=1}^{\numData} (1-\dataScalar_i) + \sum_{i=1}^{\numData} \dataScalar_i\right),\]</span></p></li>
</ul>
</section>
<section id="solution" class="slide level3">
<h3>Solution</h3>
<ul>
<li><p>Recognise that <span class="math inline">\(\sum_{i=1}^{\numData} (1-\dataScalar_i) + \sum_{i=1}^{\numData} \dataScalar_i = n\)</span> so we have <span class="math display">\[\pi = \frac{\sum_{i=1}^{\numData} \dataScalar_i}{\numData}\]</span></p></li>
<li>Estimate the probability associated with the Bernoulli by setting it to the number of observed positives, divided by the total length of <span class="math inline">\(\dataScalar\)</span>.</li>
<li>Makes intiutive sense.</li>
<li><p>What’s your best guess of probability for coin toss is heads when you get 47 heads from 100 tosses?</p></li>
</ul>

</section>
<section id="bayes-rule-reminder" class="slide level3">
<h3>Bayes’ Rule Reminder</h3>
<p><span class="math display">\[
\text{posterior} =
\frac{\text{likelihood}\times\text{prior}}{\text{marginal likelihood}}
\]</span></p>
<p>Four components:</p>
<ol type="1">
<li>Prior distribution</li>
<li>Likelihood</li>
<li>Posterior distribution</li>
<li>Marginal likelihood</li>
</ol>
</section>
<section id="naive-bayes-classifiers" class="slide level3">
<h3>Naive Bayes Classifiers</h3>
<ul>
<li>Probabilistic Machine Learning: place probability distributions (or densities) over all the variables of interest.</li>
<li><p>In <em>naive Bayes</em> this is exactly what we do.</p></li>
<li><p>Form a classification algorithm by modelling the <em>joint</em> density of our observations.</p></li>
<li><p>Need to make assumption about joint density.</p></li>
</ul>
</section>
<section id="assumptions-about-density" class="slide level3">
<h3>Assumptions about Density</h3>
<ul>
<li>Make assumptions to reduce the number of parameters we need to optimise.</li>
<li>Given label data <span class="math inline">\(\dataVector\)</span> and the inputs <span class="math inline">\(\inputMatrix\)</span> could specify joint density of all potential values of <span class="math inline">\(\dataVector\)</span> and <span class="math inline">\(\inputMatrix\)</span>, <span class="math inline">\(p(\dataVector, \inputMatrix)\)</span>.</li>
<li>If <span class="math inline">\(\inputMatrix\)</span> and <span class="math inline">\(\dataVector\)</span> are training data.</li>
<li>If <span class="math inline">\(\inputVector^*\)</span> is a test input and <span class="math inline">\(\dataScalar^*\)</span> a test location we want <span class="math display">\[
  p(\dataScalar^*|\inputMatrix, \dataVector, \inputVector^*),
  \]</span></li>
</ul>
</section>
<section id="answer-from-rules-of-probability" class="slide level3">
<h3>Answer from Rules of Probability</h3>
<ul>
<li>Compute this distribution using the product and sum rules.</li>
<li>Need the probability associated with all possible combinations of <span class="math inline">\(\dataVector\)</span> and <span class="math inline">\(\inputMatrix\)</span>.</li>
<li>There are <span class="math inline">\(2^{\numData}\)</span> possible combinations for the vector <span class="math inline">\(\dataVector\)</span></li>
<li>Probability for each of these combinations must be jointly specified along with the joint density of the matrix <span class="math inline">\(\inputMatrix\)</span>,</li>
<li>Also need to <em>extend</em> the density for any chosen test location <span class="math inline">\(\inputVector^*\)</span>.</li>
</ul>
</section>
<section id="naive-bayes-assumptions" class="slide level3">
<h3>Naive Bayes Assumptions</h3>
<ul>
<li>In <em>naive Bayes</em> we make certain simplifying assumptions that allow us to perform all of the above in practice.</li>
</ul>
<ol type="1">
<li>Data Conditional Independence</li>
<li>Feature conditional independence</li>
<li>Marginal density for <span class="math inline">\(\dataScalar\)</span>.</li>
</ol>
</section>
<section id="data-conditional-independence" class="slide level3">
<h3>Data Conditional Independence</h3>
<ul>
<li><p>Given model parameters <span class="math inline">\(\paramVector\)</span> we assume that all data points in the model are independent. <span class="math display">\[
  p(\dataScalar^*, \inputVector^*, \dataVector, \inputMatrix|\paramVector) = p(\dataScalar^*, \inputVector^*|\paramVector)\prod_{i=1}^{\numData} p(\dataScalar_i, \inputVector_i | \paramVector).
  \]</span></p></li>
<li><p>This is a conditional independence assumption.</p></li>
<li><p>We also make similar assumptions for regression (where <span class="math inline">\(\paramVector = \left\{\mappingVector,\dataStd^2\right\}\)</span>).</p></li>
<li><p>Here we assume <em>joint</em> density of <span class="math inline">\(\dataVector\)</span> and <span class="math inline">\(\inputMatrix\)</span> is independent across the data given the parameters.</p></li>
</ul>
</section>
<section id="bayes-classifier" class="slide level3">
<h3>Bayes Classifier</h3>
<p>Computing posterior distribution in this case becomes easier, this is known as the ‘Bayes classifier’.</p>
</section>
<section id="feature-conditional-independence" class="slide level3">
<h3>Feature Conditional Independence</h3>
<ul>
<li>Particular to naive Bayes: assume <em>features</em> are also conditionally independent, given param <em>and</em> the label. <span class="math display">\[p(\inputVector_i | \dataScalar_i, \paramVector) = \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i,\paramVector)\]</span> where <span class="math inline">\(\dataDim\)</span> is the dimensionality of our inputs.</li>
<li>This is known as the <em>naive Bayes</em> assumption.</li>
<li>Bayes classifier + feature conditional independence.</li>
</ul>
</section>
<section id="marginal-density-for-datascalar_i" class="slide level3">
<h3>Marginal Density for <span class="math inline">\(\dataScalar_i\)</span></h3>
<ul>
<li><p>To specify the joint distribution we also need the marginal for <span class="math inline">\(p(\dataScalar_i)\)</span> <span class="math display">\[p(\inputScalar_{i,j},\dataScalar_i| \paramVector) = p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i).\]</span></p></li>
<li><p>Because <span class="math inline">\(\dataScalar_i\)</span> is binary the <em>Bernoulli</em> density makes a suitable choice for our prior over <span class="math inline">\(\dataScalar_i\)</span>, <span class="math display">\[p(\dataScalar_i|\pi) = \pi^{\dataScalar_i} (1-\pi)^{1-\dataScalar_i}\]</span> where <span class="math inline">\(\pi\)</span> now has the interpretation as being the <em>prior</em> probability that the classification should be positive.</p></li>
</ul>
</section>
<section id="joint-density-for-naive-bayes" class="slide level3">
<h3>Joint Density for Naive Bayes</h3>
<ul>
<li>This allows us to write down the full joint density of the training data, <span class="math display">\[
  p(\dataVector, \inputMatrix|\paramVector, \pi) = \prod_{i=1}^{\numData} \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i|\pi)
  \]</span> which can now be fit by maximum likelihood.</li>
</ul>
</section>
<section id="objective-function" class="slide level3">
<h3>Objective Function</h3>
<p><span class="math display">\[\begin{align*}
\errorFunction(\paramVector, \pi)&amp; =  -\log p(\dataVector, \inputMatrix|\paramVector, \pi) \\ &amp;= -\sum_{i=1}^{\numData} \sum_{j=1}^{\dataDim} \log p(\inputScalar_{i, j}|\dataScalar_i, \paramVector) -  \sum_{i=1}^{\numData} \log p(\dataScalar_i|\pi),
\end{align*}\]</span></p>
</section>
<section id="maximum-likelihood" class="slide level3">
<h3>Maximum Likelihood</h3>
</section>
<section id="fit-prior" class="slide level3">
<h3>Fit Prior</h3>
<ul>
<li>We can minimize prior. For Bernoulli likelihood over the labels we have, <span class="math display">\[\begin{align*}
  \errorFunction(\pi) &amp; = - \sum_{i=1}^{\numData}\log p(\dataScalar_i|\pi)\\ &amp; = -\sum_{i=1}^{\numData} \dataScalar_i \log \pi - \sum_{i=1}^{\numData} (1-\dataScalar_i) \log (1-\pi)
  \end{align*}\]</span></li>
<li>Solution from above is <span class="math display">\[
  \pi = \frac{\sum_{i=1}^{\numData} \dataScalar_i}{\numData}.
  \]</span></li>
</ul>
</section>
<section id="fit-conditional" class="slide level3">
<h3>Fit Conditional</h3>
<ul>
<li>Minimize conditional distribution: <span class="math display">\[
  \errorFunction(\paramVector) = -\sum_{i=1}^{\numData} \sum_{j=1}^{\dataDim} \log p(\inputScalar_{i, j} |\dataScalar_i, \paramVector),
  \]</span></li>
<li>Implies making an assumption about it’s form.</li>
<li>The right assumption will depend on the data.</li>
<li>E.g. for real valued data, use a Gaussian <span class="math display">\[
  p(\inputScalar_{i, j} | \dataScalar_i,\paramVector) =
\frac{1}{\sqrt{2\pi \dataStd_{\dataScalar_i,j}^2}} \exp \left(-\frac{(\inputScalar_{i,j} - \mu_{\dataScalar_i,
j})^2}{\dataStd_{\dataScalar_i,j}^2}\right),
  \]</span></li>
</ul>
</section>
<section id="compute-posterior-for-test-point-label" class="slide level3">
<h3>Compute Posterior for Test Point Label</h3>
<ul>
<li>We know that <span class="math display">\[
  P(\dataScalar^*| \dataVector, \inputMatrix, \inputVector^*, \paramVector)p(\dataVector,\inputMatrix, \inputVector^*|\paramVector) = p(\dataScalar*, \dataVector, \inputMatrix,\inputVector^*| \paramVector)
  \]</span></li>
<li>This implies <span class="math display">\[
  P(\dataScalar^*| \dataVector, \inputMatrix, \inputVector^*, \paramVector) = \frac{p(\dataScalar*, \dataVector, \inputMatrix, \inputVector^*| \paramVector)}{p(\dataVector, \inputMatrix, \inputVector^*|\paramVector)}
  \]</span></li>
</ul>
</section>
<section id="compute-posterior-for-test-point-label-1" class="slide level3">
<h3>Compute Posterior for Test Point Label</h3>
<ul>
<li>From conditional independence assumptions <span class="math display">\[
  p(\dataScalar^*, \dataVector, \inputMatrix, \inputVector^*| \paramVector) = \prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*, \paramVector)p(\dataScalar^*|\pi)\prod_{i=1}^{\numData} \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i|\pi)
  \]</span></li>
<li>We also need <span class="math display">\[
  p(\dataVector, \inputMatrix, \inputVector^*|\paramVector)\]</span> which can be found from <span class="math display">\[p(\dataScalar^*, \dataVector, \inputMatrix, \inputVector^*| \paramVector)
  \]</span></li>
<li>Using the <em>sum rule</em> of probability, <span class="math display">\[
  p(\dataVector, \inputMatrix, \inputVector^*|\paramVector) = \sum_{\dataScalar^*=0}^1 p(\dataScalar^*, \dataVector, \inputMatrix, \inputVector^*| \paramVector).
  \]</span></li>
</ul>
</section>
<section id="independence-assumptions" class="slide level3">
<h3>Independence Assumptions</h3>
<ul>
<li>From independence assumptions <span class="math display">\[
  p(\dataVector, \inputMatrix, \inputVector^*| \paramVector) = \sum_{\dataScalar^*=0}^1 \prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*_i, \paramVector)p(\dataScalar^*|\pi)\prod_{i=1}^{\numData} \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i|\pi).
  \]</span></li>
<li>Substitute both forms to recover, <span class="math display">\[
  P(\dataScalar^*| \dataVector, \inputMatrix, \inputVector^*, \paramVector)  = \frac{\prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*_i, \paramVector)p(\dataScalar^*|\pi)\prod_{i=1}^{\numData} \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i|\pi)}{\sum_{\dataScalar^*=0}^1 \prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*_i, \paramVector)p(\dataScalar^*|\pi)\prod_{i=1}^{\numData} \prod_{j=1}^{\dataDim} p(\inputScalar_{i,j}|\dataScalar_i, \paramVector)p(\dataScalar_i|\pi)}
  \]</span></li>
</ul>
</section>
<section id="cancelation" class="slide level3">
<h3>Cancelation</h3>
<ul>
<li>Note training data terms cancel. <span class="math display">\[
  p(\dataScalar^*| \inputVector^*, \paramVector) = \frac{\prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*_i, \paramVector)p(\dataScalar^*|\pi)}{\sum_{\dataScalar^*=0}^1 \prod_{j=1}^{\dataDim} p(\inputScalar^*_{j}|\dataScalar^*_i, \paramVector)p(\dataScalar^*|\pi)}
  \]</span></li>
<li>This formula is also fairly straightforward to implement for different class conditional distributions.</li>
</ul>
</section>
<section id="laplace-smoothing" class="slide level3">
<h3>Laplace Smoothing</h3>
<iframe frameborder="0" scrolling="no" style="border:0px" src="http://books.google.co.uk/books?id=1YQPAAAAQAAJ&amp;pg=PA16&amp;output=embed" , width="700" height="500">
</iframe>
</section>
<section id="pseudo-counts" class="slide level3">
<h3>Pseudo Counts</h3>
<p><span class="math display">\[
\pi = \frac{\sum_{i=1}^{\numData} \dataScalar_i + 1}{\numData + 2}
\]</span></p>


</section>
<section id="naive-bayes-summary" class="slide level3">
<h3>Naive Bayes Summary</h3>
<ul>
<li>Model <em>full</em> joint distribution of data, <span class="math inline">\(p(\dataVector, \inputMatrix | \paramVector, \pi)\)</span></li>
<li>Make conditional independence assumptions about the data.</li>
<li>feature conditional independence</li>
<li>data conditional independence</li>
<li>Fast to implement, works on very large data.</li>
<li>Despite simple assumptions can perform better than expected.</li>
</ul>
</section>
<section id="other-reading" class="slide level3">
<h3>Other Reading</h3>
<ul>
<li>Chapter 5 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> up to pg 179 (Section 5.1, and 5.2 up to 5.2.2).</li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bayes:doctrine63">
<p>Bayes, T., 1763. An essay towards solving a problem in the doctrine of chances. Philosophical Transactions of the Royal Society 53, 370–418. <a href="https://doi.org/10.1098/rstl.1763.0053" class="uri">https://doi.org/10.1098/rstl.1763.0053</a></p>
</div>
<div id="ref-Bishop:book06">
<p>Bishop, C.M., 2006. Pattern recognition and machine learning. springer.</p>
</div>
<div id="ref-Laplace:memoire74">
<p>Laplace, P.S., 1774. Mémoire sur la probabilité des causes par les évènemens, in: Mémoires de Mathèmatique et de Physique, Presentés à lAcadémie Royale Des Sciences, Par Divers Savans, &amp; Lù Dans Ses Assemblées 6. pp. 621–656.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
<div id="ref-Steele:predictive12">
<p>Steele, S., Bilchik, A., Eberhardt, J., Kalina, P., Nissan, A., Johnson, E., Avital, I., Stojadinovic, A., 2012. Using machine-learned Bayesian belief networks to predict perioperative risk of clostridium difficile infection following colon surgery. Interact J Med Res 1, e6. <a href="https://doi.org/10.2196/ijmr.2131" class="uri">https://doi.org/10.2196/ijmr.2131</a></p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
