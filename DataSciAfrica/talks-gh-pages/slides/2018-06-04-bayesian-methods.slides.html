<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2018-06-04">
  <title>Bayesian Methods</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="reveal.js/css/reveal.css">
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="reveal.js/css/theme/black.css" id="theme">
  <link rel="stylesheet" href="talks.css"/>
  <!-- Printing and PDF exports -->
  <script>
    var link = document.createElement( 'link' );
    link.rel = 'stylesheet';
    link.type = 'text/css';
    link.href = window.location.search.match( /print-pdf/gi ) ? 'reveal.js/css/print/pdf.css' : 'reveal.js/css/print/paper.css';
    document.getElementsByTagName( 'head' )[0].appendChild( link );
  </script>
  <!--[if lt IE 9]>
  <script src="reveal.js/lib/js/html5shiv.js"></script>
  <![endif]-->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG" type="text/javascript"></script>
      <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          TeX: {
            extensions: ["color.js"]
          }
        });
      </script>
      <script>
  
  function setDivs(group) {
    var frame = document.getElementById("range-".concat(group)).value
    slideIndex = parseInt(frame)
    showDivs(slideIndex, group);
  }
  
  function plusDivs(n, group) {
    showDivs(slideIndex += n, group);
    document.setElementById("range-".concat(group)) = slideIndex
  }
  
  function showDivs(n,group) {
    var i;
    var x = document.getElementsByClassName(group);
    if (n > x.length) {slideIndex = 1}    
    if (n < 1) {slideIndex = x.length}
    for (i = 0; i < x.length; i++) {
       x[i].style.display = "none";  
    }
    x[slideIndex-1].style.display = "block";  
  }
      </script>
</head>
<body>
$$\newcommand{\Amatrix}{\mathbf{A}}
\newcommand{\KL}[2]{\text{KL}\left( #1\,\|\,#2 \right)}
\newcommand{\Kaast}{\kernelMatrix_{\mathbf{ \ast}\mathbf{ \ast}}}
\newcommand{\Kastu}{\kernelMatrix_{\mathbf{ \ast} \inducingVector}}
\newcommand{\Kff}{\kernelMatrix_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\Kfu}{\kernelMatrix_{\mappingFunctionVector \inducingVector}}
\newcommand{\Kuast}{\kernelMatrix_{\inducingVector \bf\ast}}
\newcommand{\Kuf}{\kernelMatrix_{\inducingVector \mappingFunctionVector}}
\newcommand{\Kuu}{\kernelMatrix_{\inducingVector \inducingVector}}
\newcommand{\Kuui}{\Kuu^{-1}}
\newcommand{\Qaast}{\mathbf{Q}_{\bf \ast \ast}}
\newcommand{\Qastf}{\mathbf{Q}_{\ast \mappingFunction}}
\newcommand{\Qfast}{\mathbf{Q}_{\mappingFunctionVector \bf \ast}}
\newcommand{\Qff}{\mathbf{Q}_{\mappingFunctionVector \mappingFunctionVector}}
\newcommand{\aMatrix}{\mathbf{A}}
\newcommand{\aScalar}{a}
\newcommand{\aVector}{\mathbf{a}}
\newcommand{\acceleration}{a}
\newcommand{\bMatrix}{\mathbf{B}}
\newcommand{\bScalar}{b}
\newcommand{\bVector}{\mathbf{b}}
\newcommand{\basisFunc}{\phi}
\newcommand{\basisFuncVector}{\boldsymbol{ \basisFunc}}
\newcommand{\basisFunction}{\phi}
\newcommand{\basisLocation}{\mu}
\newcommand{\basisMatrix}{\boldsymbol{ \Phi}}
\newcommand{\basisScalar}{\basisFunction}
\newcommand{\basisVector}{\boldsymbol{ \basisFunction}}
\newcommand{\activationFunction}{\phi}
\newcommand{\activationMatrix}{\boldsymbol{ \Phi}}
\newcommand{\activationScalar}{\basisFunction}
\newcommand{\activationVector}{\boldsymbol{ \basisFunction}}
\newcommand{\bigO}{\mathcal{O}}
\newcommand{\binomProb}{\pi}
\newcommand{\cMatrix}{\mathbf{C}}
\newcommand{\cbasisMatrix}{\hat{\boldsymbol{ \Phi}}}
\newcommand{\cdataMatrix}{\hat{\dataMatrix}}
\newcommand{\cdataScalar}{\hat{\dataScalar}}
\newcommand{\cdataVector}{\hat{\dataVector}}
\newcommand{\centeredKernelMatrix}{\mathbf{ \MakeUppercase{\centeredKernelScalar}}}
\newcommand{\centeredKernelScalar}{b}
\newcommand{\centeredKernelVector}{\centeredKernelScalar}
\newcommand{\centeringMatrix}{\mathbf{H}}
\newcommand{\chiSquaredDist}[2]{\chi_{#1}^{2}\left(#2\right)}
\newcommand{\chiSquaredSamp}[1]{\chi_{#1}^{2}}
\newcommand{\conditionalCovariance}{\boldsymbol{ \Sigma}}
\newcommand{\coregionalizationMatrix}{\mathbf{B}}
\newcommand{\coregionalizationScalar}{b}
\newcommand{\coregionalizationVector}{\mathbf{ \coregionalizationScalar}}
\newcommand{\covDist}[2]{\text{cov}_{#2}\left(#1\right)}
\newcommand{\covSamp}[1]{\text{cov}\left(#1\right)}
\newcommand{\covarianceScalar}{c}
\newcommand{\covarianceVector}{\mathbf{ \covarianceScalar}}
\newcommand{\covarianceMatrix}{\mathbf{C}}
\newcommand{\covarianceMatrixTwo}{\boldsymbol{ \Sigma}}
\newcommand{\croupierScalar}{s}
\newcommand{\croupierVector}{\mathbf{ \croupierScalar}}
\newcommand{\croupierMatrix}{\mathbf{ \MakeUppercase{\croupierScalar}}}
\newcommand{\dataDim}{p}
\newcommand{\dataIndex}{i}
\newcommand{\dataIndexTwo}{j}
\newcommand{\dataMatrix}{\mathbf{Y}}
\newcommand{\dataScalar}{y}
\newcommand{\dataSet}{\mathcal{D}}
\newcommand{\dataStd}{\sigma}
\newcommand{\dataVector}{\mathbf{ \dataScalar}}
\newcommand{\decayRate}{d}
\newcommand{\degreeMatrix}{\mathbf{ \MakeUppercase{\degreeScalar}}}
\newcommand{\degreeScalar}{d}
\newcommand{\degreeVector}{\mathbf{ \degreeScalar}}
% Already defined by latex
%\newcommand{\det}[1]{\left|#1\right|}
\newcommand{\diag}[1]{\text{diag}\left(#1\right)}
\newcommand{\diagonalMatrix}{\mathbf{D}}
\newcommand{\diff}[2]{\frac{\text{d}#1}{\text{d}#2}}
\newcommand{\diffTwo}[2]{\frac{\text{d}^2#1}{\text{d}#2^2}}
\newcommand{\displacement}{x}
\newcommand{\displacementVector}{\textbf{\displacement}}
\newcommand{\distanceMatrix}{\mathbf{ \MakeUppercase{\distanceScalar}}}
\newcommand{\distanceScalar}{d}
\newcommand{\distanceVector}{\mathbf{ \distanceScalar}}
\newcommand{\eigenvaltwo}{\ell}
\newcommand{\eigenvaltwoMatrix}{\mathbf{L}}
\newcommand{\eigenvaltwoVector}{\mathbf{l}}
\newcommand{\eigenvalue}{\lambda}
\newcommand{\eigenvalueMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\eigenvalueVector}{\boldsymbol{ \lambda}}
\newcommand{\eigenvector}{\mathbf{ \eigenvectorScalar}}
\newcommand{\eigenvectorMatrix}{\mathbf{U}}
\newcommand{\eigenvectorScalar}{u}
\newcommand{\eigenvectwo}{\mathbf{v}}
\newcommand{\eigenvectwoMatrix}{\mathbf{V}}
\newcommand{\eigenvectwoScalar}{v}
\newcommand{\entropy}[1]{\mathcal{H}\left(#1\right)}
\newcommand{\errorFunction}{E}
\newcommand{\expDist}[2]{\left<#1\right>_{#2}}
\newcommand{\expSamp}[1]{\left<#1\right>}
\newcommand{\expectation}[1]{\left\langle #1 \right\rangle }
\newcommand{\expectationDist}[2]{\left\langle #1 \right\rangle _{#2}}
\newcommand{\expectedDistanceMatrix}{\mathcal{D}}
\newcommand{\eye}{\mathbf{I}}
\newcommand{\fantasyDim}{r}
\newcommand{\fantasyMatrix}{\mathbf{ \MakeUppercase{\fantasyScalar}}}
\newcommand{\fantasyScalar}{z}
\newcommand{\fantasyVector}{\mathbf{ \fantasyScalar}}
\newcommand{\featureStd}{\varsigma}
\newcommand{\gammaCdf}[3]{\mathcal{GAMMA CDF}\left(#1|#2,#3\right)}
\newcommand{\gammaDist}[3]{\mathcal{G}\left(#1|#2,#3\right)}
\newcommand{\gammaSamp}[2]{\mathcal{G}\left(#1,#2\right)}
\newcommand{\gaussianDist}[3]{\mathcal{N}\left(#1|#2,#3\right)}
\newcommand{\gaussianSamp}[2]{\mathcal{N}\left(#1,#2\right)}
\newcommand{\given}{|}
\newcommand{\half}{\frac{1}{2}}
\newcommand{\heaviside}{H}
\newcommand{\hiddenMatrix}{\mathbf{ \MakeUppercase{\hiddenScalar}}}
\newcommand{\hiddenScalar}{h}
\newcommand{\hiddenVector}{\mathbf{ \hiddenScalar}}
\newcommand{\identityMatrix}{\eye}
\newcommand{\inducingInputScalar}{z}
\newcommand{\inducingInputVector}{\mathbf{ \inducingInputScalar}}
\newcommand{\inducingInputMatrix}{\mathbf{Z}}
\newcommand{\inducingScalar}{u}
\newcommand{\inducingVector}{\mathbf{ \inducingScalar}}
\newcommand{\inducingMatrix}{\mathbf{U}}
\newcommand{\inlineDiff}[2]{\text{d}#1/\text{d}#2}
\newcommand{\inputDim}{q}
\newcommand{\inputMatrix}{\mathbf{X}}
\newcommand{\inputScalar}{x}
\newcommand{\inputSpace}{\mathcal{X}}
\newcommand{\inputVals}{\inputVector}
\newcommand{\inputVector}{\mathbf{ \inputScalar}}
\newcommand{\iterNum}{k}
\newcommand{\kernel}{\kernelScalar}
\newcommand{\kernelMatrix}{\mathbf{K}}
\newcommand{\kernelScalar}{k}
\newcommand{\kernelVector}{\mathbf{ \kernelScalar}}
\newcommand{\kff}{\kernelScalar_{\mappingFunction \mappingFunction}}
\newcommand{\kfu}{\kernelVector_{\mappingFunction \inducingScalar}}
\newcommand{\kuf}{\kernelVector_{\inducingScalar \mappingFunction}}
\newcommand{\kuu}{\kernelVector_{\inducingScalar \inducingScalar}}
\newcommand{\lagrangeMultiplier}{\lambda}
\newcommand{\lagrangeMultiplierMatrix}{\boldsymbol{ \Lambda}}
\newcommand{\lagrangian}{L}
\newcommand{\laplacianFactor}{\mathbf{ \MakeUppercase{\laplacianFactorScalar}}}
\newcommand{\laplacianFactorScalar}{m}
\newcommand{\laplacianFactorVector}{\mathbf{ \laplacianFactorScalar}}
\newcommand{\laplacianMatrix}{\mathbf{L}}
\newcommand{\laplacianScalar}{\ell}
\newcommand{\laplacianVector}{\mathbf{ \ell}}
\newcommand{\latentDim}{q}
\newcommand{\latentDistanceMatrix}{\boldsymbol{ \Delta}}
\newcommand{\latentDistanceScalar}{\delta}
\newcommand{\latentDistanceVector}{\boldsymbol{ \delta}}
\newcommand{\latentForce}{f}
\newcommand{\latentFunction}{u}
\newcommand{\latentFunctionVector}{\mathbf{ \latentFunction}}
\newcommand{\latentFunctionMatrix}{\mathbf{ \MakeUppercase{\latentFunction}}}
\newcommand{\latentIndex}{j}
\newcommand{\latentScalar}{z}
\newcommand{\latentVector}{\mathbf{ \latentScalar}}
\newcommand{\latentMatrix}{\mathbf{Z}}
\newcommand{\learnRate}{\eta}
\newcommand{\lengthScale}{\ell}
\newcommand{\rbfWidth}{\ell}
\newcommand{\likelihoodBound}{\mathcal{L}}
\newcommand{\likelihoodFunction}{L}
\newcommand{\locationScalar}{\mu}
\newcommand{\locationVector}{\boldsymbol{ \locationScalar}}
\newcommand{\locationMatrix}{\mathbf{M}}
\newcommand{\variance}[1]{\text{var}\left( #1 \right)}
\newcommand{\mappingFunction}{f}
\newcommand{\mappingFunctionMatrix}{\mathbf{F}}
\newcommand{\mappingFunctionTwo}{g}
\newcommand{\mappingFunctionTwoMatrix}{\mathbf{G}}
\newcommand{\mappingFunctionTwoVector}{\mathbf{ \mappingFunctionTwo}}
\newcommand{\mappingFunctionVector}{\mathbf{ \mappingFunction}}
\newcommand{\scaleScalar}{s}
\newcommand{\mappingScalar}{w}
\newcommand{\mappingVector}{\mathbf{ \mappingScalar}}
\newcommand{\mappingMatrix}{\mathbf{W}}
\newcommand{\mappingScalarTwo}{v}
\newcommand{\mappingVectorTwo}{\mathbf{ \mappingScalarTwo}}
\newcommand{\mappingMatrixTwo}{\mathbf{V}}
\newcommand{\maxIters}{K}
\newcommand{\meanMatrix}{\mathbf{M}}
\newcommand{\meanScalar}{\mu}
\newcommand{\meanTwoMatrix}{\mathbf{M}}
\newcommand{\meanTwoScalar}{m}
\newcommand{\meanTwoVector}{\mathbf{ \meanTwoScalar}}
\newcommand{\meanVector}{\boldsymbol{ \meanScalar}}
\newcommand{\mrnaConcentration}{m}
\newcommand{\naturalFrequency}{\omega}
\newcommand{\neighborhood}[1]{\mathcal{N}\left( #1 \right)}
\newcommand{\neilurl}{http://inverseprobability.com/}
\newcommand{\noiseMatrix}{\boldsymbol{ E}}
\newcommand{\noiseScalar}{\epsilon}
\newcommand{\noiseVector}{\boldsymbol{ \epsilon}}
\newcommand{\norm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\normalizedLaplacianMatrix}{\hat{\mathbf{L}}}
\newcommand{\normalizedLaplacianScalar}{\hat{\ell}}
\newcommand{\normalizedLaplacianVector}{\hat{\mathbf{ \ell}}}
\newcommand{\numActive}{m}
\newcommand{\numBasisFunc}{m}
\newcommand{\numComponents}{m}
\newcommand{\numComps}{K}
\newcommand{\numData}{n}
\newcommand{\numFeatures}{K}
\newcommand{\numHidden}{h}
\newcommand{\numInducing}{m}
\newcommand{\numLayers}{\ell}
\newcommand{\numNeighbors}{K}
\newcommand{\numSequences}{s}
\newcommand{\numSuccess}{s}
\newcommand{\numTasks}{m}
\newcommand{\numTime}{T}
\newcommand{\numTrials}{S}
\newcommand{\outputIndex}{j}
\newcommand{\paramVector}{\boldsymbol{ \theta}}
\newcommand{\parameterMatrix}{\boldsymbol{ \Theta}}
\newcommand{\parameterScalar}{\theta}
\newcommand{\parameterVector}{\boldsymbol{ \parameterScalar}}
\newcommand{\partDiff}[2]{\frac{\partial#1}{\partial#2}}
\newcommand{\precisionScalar}{j}
\newcommand{\precisionVector}{\mathbf{ \precisionScalar}}
\newcommand{\precisionMatrix}{\mathbf{J}}
\newcommand{\pseudotargetScalar}{\widetilde{y}}
\newcommand{\pseudotargetVector}{\mathbf{ \pseudotargetScalar}}
\newcommand{\pseudotargetMatrix}{\mathbf{ \widetilde{Y}}}
\newcommand{\rank}[1]{\text{rank}\left(#1\right)}
\newcommand{\rayleighDist}[2]{\mathcal{R}\left(#1|#2\right)}
\newcommand{\rayleighSamp}[1]{\mathcal{R}\left(#1\right)}
\newcommand{\responsibility}{r}
\newcommand{\rotationScalar}{r}
\newcommand{\rotationVector}{\mathbf{ \rotationScalar}}
\newcommand{\rotationMatrix}{\mathbf{R}}
\newcommand{\sampleCovScalar}{s}
\newcommand{\sampleCovVector}{\mathbf{ \sampleCovScalar}}
\newcommand{\sampleCovMatrix}{\mathbf{s}}
\newcommand{\scalarProduct}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\sign}[1]{\text{sign}\left(#1\right)}
\newcommand{\sigmoid}[1]{\sigma\left(#1\right)}
\newcommand{\singularvalue}{\ell}
\newcommand{\singularvalueMatrix}{\mathbf{L}}
\newcommand{\singularvalueVector}{\mathbf{l}}
\newcommand{\sorth}{\mathbf{u}}
\newcommand{\spar}{\lambda}
\newcommand{\trace}[1]{\text{tr}\left(#1\right)}
\newcommand{\BasalRate}{B}
\newcommand{\DampingCoefficient}{C}
\newcommand{\DecayRate}{D}
\newcommand{\Displacement}{X}
\newcommand{\LatentForce}{F}
\newcommand{\Mass}{M}
\newcommand{\Sensitivity}{S}
\newcommand{\basalRate}{b}
\newcommand{\dampingCoefficient}{c}
\newcommand{\mass}{m}
\newcommand{\sensitivity}{s}
\newcommand{\springScalar}{\kappa}
\newcommand{\springVector}{\boldsymbol{ \kappa}}
\newcommand{\springMatrix}{\boldsymbol{ \mathcal{K}}}
\newcommand{\tfConcentration}{p}
\newcommand{\tfDecayRate}{\delta}
\newcommand{\tfMrnaConcentration}{f}
\newcommand{\tfVector}{\mathbf{ \tfConcentration}}
\newcommand{\velocity}{v}
\newcommand{\sufficientStatsScalar}{g}
\newcommand{\sufficientStatsVector}{\mathbf{ \sufficientStatsScalar}}
\newcommand{\sufficientStatsMatrix}{\mathbf{G}}
\newcommand{\switchScalar}{s}
\newcommand{\switchVector}{\mathbf{ \switchScalar}}
\newcommand{\switchMatrix}{\mathbf{S}}
\newcommand{\tr}[1]{\text{tr}\left(#1\right)}
\newcommand{\loneNorm}[1]{\left\Vert #1 \right\Vert_1}
\newcommand{\ltwoNorm}[1]{\left\Vert #1 \right\Vert_2}
\newcommand{\onenorm}[1]{\left\vert#1\right\vert_1}
\newcommand{\twonorm}[1]{\left\Vert #1 \right\Vert}
\newcommand{\vScalar}{v}
\newcommand{\vVector}{\mathbf{v}}
\newcommand{\vMatrix}{\mathbf{V}}
\newcommand{\varianceDist}[2]{\text{var}_{#2}\left( #1 \right)}
% Already defined by latex
%\newcommand{\vec}{#1:}
\newcommand{\vecb}[1]{\left(#1\right):}
\newcommand{\weightScalar}{w}
\newcommand{\weightVector}{\mathbf{ \weightScalar}}
\newcommand{\weightMatrix}{\mathbf{W}}
\newcommand{\weightedAdjacencyMatrix}{\mathbf{A}}
\newcommand{\weightedAdjacencyScalar}{a}
\newcommand{\weightedAdjacencyVector}{\mathbf{ \weightedAdjacencyScalar}}
\newcommand{\onesVector}{\mathbf{1}}
\newcommand{\zerosVector}{\mathbf{0}}
$$
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Bayesian Methods</h1>
  <p class="author" style="text-align:center"><a href="http://inverseprobability.com">Neil D. Lawrence</a></p>
  <p class="date" style="text-align:center"><time>2018-06-04</time></p>
  <p class="venue" style="text-align:center">Data Science Africa, Nyeri, Kenya</p>
</section>

<section class="slide level3">

<!-- SECTION What is Machine Learning? -->
</section>
<section id="what-is-machine-learning" class="slide level3">
<h3>What is Machine Learning?</h3>
<div class="fragment">
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
</div>
<div class="fragment">
<ul>
<li><strong>data</strong> : observations, could be actively or passively acquired (meta-data).</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>model</strong> : assumptions, based on previous experience (other data! transfer learning etc), or beliefs about the regularities of the universe. Inductive bias.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>prediction</strong> : an action to be taken or a categorization or a quality score.</li>
</ul>
</div>
<div class="fragment">
<ul>
<li>Royal Society Report: <a href="https://royalsociety.org/~/media/policy/projects/machine-learning/publications/machine-learning-report.pdf">Machine Learning: Power and Promise of Computers that Learn by Example</a></li>
</ul>
</div>
</section>
<section id="what-is-machine-learning-1" class="slide level3">
<h3>What is Machine Learning?</h3>
<p><span class="math display">\[\text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<div class="fragment">
<ul>
<li>To combine data with a model need:</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>a prediction function</strong> <span class="math inline">\(\mappingFunction(\cdot)\)</span> includes our beliefs about the regularities of the universe</li>
</ul>
</div>
<div class="fragment">
<ul>
<li><strong>an objective function</strong> <span class="math inline">\(\errorFunction(\cdot)\)</span> defines the cost of misprediction.</li>
</ul>
</div>
</section>
<section id="olympic-marathon-data" class="slide level3">
<h3>Olympic Marathon Data</h3>
<table>
<tr>
<td width="70%">
<ul>
<li><p>Gold medal times for Olympic Marathon since 1896.</p></li>
<li><p>Marathons before 1924 didn’t have a standardised distance.</p></li>
<li><p>Present results using pace per km.</p></li>
<li>In 1904 Marathon was badly organised leading to very slow times.</li>
</ul>
</td>
<td width="30%">
<img src="../slides/diagrams/Stephen_Kiprotich.jpg" alt="image" /> <small>Image from Wikimedia Commons <a href="http://bit.ly/16kMKHQ" class="uri">http://bit.ly/16kMKHQ</a></small>
</td>
</tr>
</table>
</section>
<section id="olympic-marathon-data-1" class="slide level3">
<h3>Olympic Marathon Data</h3>
<object class="svgplot " align data="../slides/diagrams/datasets/olympic-marathon.svg">
</object>
</section>
<section id="regression-linear-releationship" class="slide level3">
<h3>Regression: Linear Releationship</h3>
<p><span class="math display">\[\dataScalar_i = m \inputScalar_i + c\]</span></p>
<ul>
<li><p><span class="math inline">\(\dataScalar_i\)</span> : winning pace.</p></li>
<li><p><span class="math inline">\(\inputScalar_i\)</span> : year of Olympics.</p></li>
<li><p><span class="math inline">\(m\)</span> : rate of improvement over time.</p></li>
<li><p><span class="math inline">\(c\)</span> : winning time at year 0.</p></li>
</ul>
<!-- SECTION Overdetermined System -->
</section>
<section id="section" class="slide level3">
<h3></h3>
<script>
showDivs(1, 'over_determined_system');
</script>
<button onclick="plusDivs(-1, 'over_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'over_determined_system')">
❯
</button>
<p><input id="range-over_determined_system" type="range" min="1" max="8" value="1" onchange="setDivs('over_determined_system')" oninput="setDivs('over_determined_system')"> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system001.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system002.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system003.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system004.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system005.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system006.svg"></object> <object class="svgplot over_determined_system" align="" data="../slides/diagrams/ml/over_determined_system007.svg"></object></p>
</section>
<section id="datascalar-minputscalar-c" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span></p>
<p><span class="math display">\[2.5 = 2m + c\]</span></p>
</div>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/ml/Pierre-Simon_Laplace.png" width="30%" align="" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-2" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/laplacesDeterminismFrench.png" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-3" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/laplacesDeterminismEnglish.png" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="section-4" class="slide level3">
<h3></h3>
<p><img class="" src="../slides/diagrams/philosophicaless00lapliala.png" width="60%" align="center" style="background:none; border:none; box-shadow:none;"></p>
</section>
<section id="datascalar-minputscalar-c-noisescalar" class="slide level3">
<h3><span class="math inline">\(\dataScalar = m\inputScalar + c + \noiseScalar\)</span></h3>
<div class="fragment">
<p>point 1: <span class="math inline">\(\inputScalar = 1\)</span>, <span class="math inline">\(\dataScalar=3\)</span> <span class="math display">\[
3 = m + c + \noiseScalar_1
\]</span></p>
</div>
<div class="fragment">
<p>point 2: <span class="math inline">\(\inputScalar = 3\)</span>, <span class="math inline">\(\dataScalar=1\)</span> <span class="math display">\[
1 = 3m + c + \noiseScalar_2
\]</span></p>
</div>
<div class="fragment">
<p>point 3: <span class="math inline">\(\inputScalar = 2\)</span>, <span class="math inline">\(\dataScalar=2.5\)</span> <span class="math display">\[
2.5 = 2m + c + \noiseScalar_3
\]</span></p>
</div>
</section>
<section id="a-probabilistic-process" class="slide level3">
<h3>A Probabilistic Process</h3>
<div class="fragment">
<p>Set the mean of Gaussian to be a function. <span class="math display">\[p
\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i-\mappingFunction\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
</div>
</section>
<section id="the-gaussian-density" class="slide level3">
<h3>The Gaussian Density</h3>
<ul>
<li>Perhaps the most common probability density.</li>
</ul>
<div class="fragment">
<p><span class="math display">\[\begin{align}
  p(\dataScalar| \meanScalar, \dataStd^2) &amp; = \frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{(\dataScalar - \meanScalar)^2}{2\dataStd^2}\right)\\&amp; \buildrel\triangle\over = \gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2}
  \end{align}\]</span></p>
</div>
</section>
<section id="gaussian-density" class="slide level3">
<h3>Gaussian Density</h3>
<object class="svgplot " align data="../slides/diagrams/ml/gaussian_of_height.svg">
</object>
<center>
<em>The Gaussian PDF with <span class="math inline">\({\meanScalar}=1.7\)</span> and variance <span class="math inline">\({\dataStd}^2=0.0225\)</span>. Mean shown as cyan line. It could represent the heights of a population of students. </em>
</center>
</section>
<section id="gaussian-density-1" class="slide level3">
<h3>Gaussian Density</h3>
<p><large><span class="math display">\[
\gaussianDist{\dataScalar}{\meanScalar}{\dataStd^2} = \frac{1}{\sqrt{2\pi\dataStd^2}} \exp\left(-\frac{(\dataScalar-\meanScalar)^2}{2\dataStd^2}\right)
\]</span></large></p>
<div class="fragment">
<center>
<span class="math inline">\(\dataStd^2\)</span> is the variance of the density and <span class="math inline">\(\meanScalar\)</span> is the mean.
</center>
</div>
</section>
<section id="two-important-gaussian-properties" class="slide level3">
<h3>Two Important Gaussian Properties</h3>
</section>
<section id="sum-of-gaussians" class="slide level3">
<h3>Sum of Gaussians</h3>
<div class="fragment">
<p><span align="left">Sum of Gaussian variables is also Gaussian.</span></p>
<p><span class="math display">\[\dataScalar_i \sim \gaussianSamp{\meanScalar_i}{\sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the sum is distributed as</span></p>
<p><span class="math display">\[\sum_{i=1}^{\numData} \dataScalar_i \sim \gaussianSamp{\sum_{i=1}^\numData \meanScalar_i}{\sum_{i=1}^\numData \sigma_i^2}\]</span></p>
</div>
<div class="fragment">
<p><small>(<em>Aside</em>: As sum increases, sum of non-Gaussian, finite variance variables is also Gaussian because of <a href="https://en.wikipedia.org/wiki/Central_limit_theorem">central limit theorem</a>.)</small></p>
</div>
</section>
<section id="scaling-a-gaussian" class="slide level3">
<h3>Scaling a Gaussian</h3>
<div class="fragment">
<p><span align="left">Scaling a Gaussian leads to a Gaussian.</span></p>
</div>
<div class="fragment">
<p><span class="math display">\[\dataScalar \sim \gaussianSamp{\meanScalar}{\sigma^2}\]</span></p>
</div>
<div class="fragment">
<p><span align="left">And the scaled variable is distributed as</span></p>
<p><span class="math display">\[\mappingScalar \dataScalar \sim \gaussianSamp{\mappingScalar\meanScalar}{\mappingScalar^2 \sigma^2}.\]</span></p>
<!-- SECTION Laplace's Idea -->
</div>
</section>
<section id="a-probabilistic-process-1" class="slide level3">
<h3>A Probabilistic Process</h3>
<p>Set the mean of Gaussian to be a function.</p>
<div class="fragment">
<p><span class="math display">\[p\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-f\left(\inputScalar_i\right)\right)^{2}}{2\dataStd^2}\right).\]</span></p>
</div>
<div class="fragment">
<p>This gives us a ‘noisy function’.</p>
</div>
<div class="fragment">
<p>This is known as a stochastic process.</p>
</div>
</section>
<section id="height-as-a-function-of-weight" class="slide level3">
<h3>Height as a Function of Weight</h3>
<p>In the standard Gaussian, parametized by mean and variance.</p>
<p>Make the mean a linear function of an <em>input</em>.</p>
<p>This leads to a regression model. <span class="math display">\[
\begin{align*}
  \dataScalar_i=&amp;\mappingFunction\left(\inputScalar_i\right)+\noiseScalar_i,\\
         \noiseScalar_i \sim &amp; \gaussianSamp{0}{\dataStd^2}.
  \end{align*}
\]</span></p>
<p>Assume <span class="math inline">\(\dataScalar_i\)</span> is height and <span class="math inline">\(\inputScalar_i\)</span> is weight.</p>
</section>
<section id="data-point-likelihood" class="slide level3">
<h3>Data Point Likelihood</h3>
<p>Likelihood of an individual data point <span class="math display">\[
p\left(\dataScalar_i|\inputScalar_i,m,c\right)=\frac{1}{\sqrt{2\pi \dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span> Parameters are gradient, <span class="math inline">\(m\)</span>, offset, <span class="math inline">\(c\)</span> of the function and noise variance <span class="math inline">\(\dataStd^2\)</span>.</p>
</section>
<section id="data-set-likelihood" class="slide level3">
<h3>Data Set Likelihood</h3>
<p>If the noise, <span class="math inline">\(\epsilon_i\)</span> is sampled independently for each data point. Each data point is independent (given <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>). For <em>independent</em> variables: <span class="math display">\[
p(\dataVector) = \prod_{i=1}^\numData p(\dataScalar_i)
\]</span> <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \prod_{i=1}^\numData p(\dataScalar_i|\inputScalar_i, m, c)
\]</span></p>
</section>
<section id="for-gaussian" class="slide level3">
<h3>For Gaussian</h3>
<p>i.i.d. assumption <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \prod_{i=1}^\numData \frac{1}{\sqrt{2\pi \dataStd^2}}\exp \left(-\frac{\left(\dataScalar_i- m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span> <span class="math display">\[
p(\dataVector|\inputVector, m, c) = \frac{1}{\left(2\pi \dataStd^2\right)^{\frac{\numData}{2}}}\exp\left(-\frac{\sum_{i=1}^\numData\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}\right).
\]</span></p>
</section>
<section id="log-likelihood-function" class="slide level3">
<h3>Log Likelihood Function</h3>
<ul>
<li>Normally work with the log likelihood: <span class="math display">\[
L(m,c,\dataStd^{2})=-\frac{\numData}{2}\log 2\pi -\frac{\numData}{2}\log \dataStd^2 -\sum_{i=1}^{\numData}\frac{\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}}{2\dataStd^2}.
\]</span></li>
</ul>
</section>
<section id="consistency-of-maximum-likelihood" class="slide level3">
<h3>Consistency of Maximum Likelihood</h3>
<ul>
<li>If data was really generated according to probability we specified.</li>
<li>Correct parameters will be recovered in limit as <span class="math inline">\(\numData \rightarrow \infty\)</span>.</li>
<li>This can be proven through sample based approximations (law of large numbers) of “KL divergences”.</li>
<li>Mainstay of classical statistics.</li>
</ul>
</section>
<section id="probabilistic-interpretation-of-the-error-function" class="slide level3">
<h3>Probabilistic Interpretation of the Error Function</h3>
<ul>
<li>Probabilistic Interpretation for Error Function is Negative Log Likelihood.</li>
<li><em>Minimizing</em> error function is equivalent to <em>maximizing</em> log likelihood.</li>
<li>Maximizing <em>log likelihood</em> is equivalent to maximizing the <em>likelihood</em> because <span class="math inline">\(\log\)</span> is monotonic.</li>
<li>Probabilistic interpretation: Minimizing error function is equivalent to maximum likelihood with respect to parameters.</li>
</ul>
</section>
<section id="error-function" class="slide level3">
<h3>Error Function</h3>
<ul>
<li>Negative log likelihood is the error function leading to an error function <span class="math display">\[\errorFunction(m,c,\dataStd^{2})=\frac{\numData}{2}\log \dataStd^2+\frac{1}{2\dataStd^2}\sum _{i=1}^{\numData}\left(\dataScalar_i-m\inputScalar_i-c\right)^{2}.\]</span></li>
<li>Learning proceeds by minimizing this error function for the data set provided.</li>
</ul>
</section>
<section id="connection-sum-of-squares-error" class="slide level3">
<h3>Connection: Sum of Squares Error</h3>
<ul>
<li>Ignoring terms which don’t depend on <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> gives <span class="math display">\[\errorFunction(m, c) \propto \sum_{i=1}^\numData (\dataScalar_i - \mappingFunction(\inputScalar_i))^2\]</span> where <span class="math inline">\(\mappingFunction(\inputScalar_i) = m\inputScalar_i + c\)</span>.</li>
<li>This is known as the <em>sum of squares</em> error function.</li>
<li>Commonly used and is closely associated with the Gaussian likelihood.</li>
</ul>
</section>
<section id="reminder" class="slide level3">
<h3>Reminder</h3>
<ul>
<li>Two functions involved:
<ul>
<li><em>Prediction function</em>: <span class="math inline">\(\mappingFunction(\inputScalar_i)\)</span></li>
<li>Error, or <em>Objective function</em>: <span class="math inline">\(\errorFunction(m, c)\)</span></li>
</ul></li>
<li>Error function depends on parameters through prediction function.</li>
</ul>
</section>
<section id="mathematical-interpretation" class="slide level3">
<h3>Mathematical Interpretation</h3>
<ul>
<li>What is the mathematical interpretation?</li>
<li>There is a cost function.
<ul>
<li>It expresses mismatch between your prediction and reality. <span class="math display">\[
  \errorFunction(m, c)=\sum_{i=1}^\numData \left(\dataScalar_i - m\inputScalar_i-c\right)^2
  \]</span></li>
<li>This is known as the sum of squares error.</li>
</ul></li>
</ul>
<!-- SECTION Sum of Squares Error -->
<!-- SECTION Linear Algebra -->
</section>
<section id="coordinate-descent" class="slide level3">
<h3>Coordinate Descent</h3>
</section>
<section id="learning-is-optimization" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Learning is minimization of the cost function.</li>
<li>At the minima the gradient is zero.</li>
<li>Coordinate ascent, find gradient in each coordinate and set to zero. <span class="math display">\[\frac{\text{d}\errorFunction(c)}{\text{d}c} = -2\sum_{i=1}^\numData \left(\dataScalar_i- m \inputScalar_i - c \right)\]</span> <span class="math display">\[0 = -2\sum_{i=1}^\numData\left(\dataScalar_i- m\inputScalar_i - c \right)\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-1" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Fixed point equations <span class="math display">\[0 = -2\sum_{i=1}^\numData \dataScalar_i +2\sum_{i=1}^\numData m \inputScalar_i +2n c\]</span> <span class="math display">\[c = \frac{\sum_{i=1}^\numData \left(\dataScalar_i - m\inputScalar_i\right)}{\numData}\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-2" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Learning is minimization of the cost function.</li>
<li>At the minima the gradient is zero.</li>
<li>Coordinate ascent, find gradient in each coordinate and set to zero. <span class="math display">\[\frac{\text{d}\errorFunction(m)}{\text{d}m} = -2\sum_{i=1}^\numData \inputScalar_i\left(\dataScalar_i- m \inputScalar_i - c \right)\]</span> <span class="math display">\[0 = -2\sum_{i=1}^\numData \inputScalar_i \left(\dataScalar_i-m \inputScalar_i - c \right)\]</span></li>
</ul>
</section>
<section id="learning-is-optimization-3" class="slide level3">
<h3>Learning is Optimization</h3>
<ul>
<li>Fixed point equations <span class="math display">\[0 = -2\sum_{i=1}^\numData \inputScalar_i\dataScalar_i+2\sum_{i=1}^\numData m \inputScalar_i^2+2\sum_{i=1}^\numData c\inputScalar_i\]</span> <span class="math display">\[m  =    \frac{\sum_{i=1}^\numData \left(\dataScalar_i -c\right)\inputScalar_i}{\sum_{i=1}^\numData\inputScalar_i^2}\]</span></li>
</ul>
<p><span class="math display">\[m^* = \frac{\sum_{i=1}^\numData (\dataScalar_i - c)\inputScalar_i}{\sum_{i=1}^\numData \inputScalar_i^2}\]</span></p>
</section>
<section id="fixed-point-updates" class="slide level3">
<h3>Fixed Point Updates</h3>
<p><span align="left">Worked example.</span> <span class="math display">\[
\begin{aligned}
    c^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\left(\dataScalar_i-m^{*}\inputScalar_i\right)}{\numData},\\
    m^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\inputScalar_i\left(\dataScalar_i-c^{*}\right)}{\sum _{i=1}^{\numData}\inputScalar_i^{2}},\\
\left.\dataStd^2\right.^{*}=&amp;\frac{\sum
_{i=1}^{\numData}\left(\dataScalar_i-m^{*}\inputScalar_i-c^{*}\right)^{2}}{\numData}
\end{aligned}
\]</span></p>
</section>
<section id="important-concepts-not-covered" class="slide level3">
<h3>Important Concepts Not Covered</h3>
<ul>
<li>Other optimization methods:
<ul>
<li>Second order methods, conjugate gradient, quasi-Newton and Newton.</li>
</ul></li>
<li>Effective heuristics such as momentum.</li>
<li>Local vs global solutions.</li>
</ul>
</section>
<section id="reading" class="slide level3">
<h3>Reading</h3>
<ul>
<li>Section 1.1-1.2 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> for fitting linear models.</li>
<li>Section 1.2.5 of <span class="citation" data-cites="Bishop:book06">Bishop (2006)</span> up to equation 1.65.</li>
</ul>
</section>
<section id="multi-dimensional-inputs" class="slide level3">
<h3>Multi-dimensional Inputs</h3>
<ul>
<li>Multivariate functions involve more than one input.</li>
<li>Height might be a function of weight and gender.</li>
<li>There could be other contributory factors.</li>
<li>Place these factors in a feature vector <span class="math inline">\(\inputVector_i\)</span>.</li>
<li>Linear function is now defined as <span class="math display">\[\mappingFunction(\inputVector_i) = \sum_{j=1}^p w_j \inputScalar_{i, j} + c\]</span></li>
</ul>
</section>
<section id="vector-notation" class="slide level3">
<h3>Vector Notation</h3>
<ul>
<li>Write in vector notation, <span class="math display">\[\mappingFunction(\inputVector_i) = \mappingVector^\top \inputVector_i + c\]</span></li>
<li>Can absorb <span class="math inline">\(c\)</span> into <span class="math inline">\(\mappingVector\)</span> by assuming extra input <span class="math inline">\(\inputScalar_0\)</span> which is always 1. <span class="math display">\[\mappingFunction(\inputVector_i) = \mappingVector^\top \inputVector_i\]</span></li>
</ul>
</section>
<section id="objective-functions-and-regression" class="slide level3">
<h3>Objective Functions and Regression</h3>
<ul>
<li>Classification: map feature to class label.</li>
<li><p>Regression: map feature to real value our <em>prediction function</em> is</p>
<p><span class="math display">\[\mappingFunction(\inputScalar_i) = m\inputScalar_i + c\]</span></p></li>
<li><p>Need an <em>algorithm</em> to fit it.</p></li>
<li><p>Least squares: minimize an error.</p></li>
</ul>
<p><span class="math display">\[\errorFunction(m, c) = \sum_{i=1}^\numData (\dataScalar_i * \mappingFunction(\inputScalar_i))^2\]</span></p>
</section>
<section id="regression" class="slide level3">
<h3>Regression</h3>
<ul>
<li>Create an artifical data set.</li>
</ul>
<p>We now need to decide on a <em>true</em> value for <span class="math inline">\(m\)</span> and a <em>true</em> value for <span class="math inline">\(c\)</span> to use for generating the data.</p>
<p>We can use these values to create our artificial data. The formula <span class="math display">\[\dataScalar_i = m\inputScalar_i + c\]</span> is translated to code as follows:</p>
</section>
<section id="plot-of-data" class="slide level3">
<h3>Plot of Data</h3>
<p>We can now plot the artifical data we’ve created.</p>
<object class="svgplot " align data="../slides/diagrams/ml/regression.svg">
</object>
<p>These points lie exactly on a straight line, that’s not very realistic, let’s corrupt them with a bit of Gaussian ‘noise’.</p>
</section>
<section id="noise-corrupted-plot" class="slide level3">
<h3>Noise Corrupted Plot</h3>
<object class="svgplot " align data="../slides/diagrams/ml/regression_noise.svg">
</object>
</section>
<section id="contour-plot-of-error-function" class="slide level3">
<h3>Contour Plot of Error Function</h3>
<ul>
<li><p>Visualise the error function surface, create vectors of values.</p></li>
<li><p>create a grid of values to evaluate the error function in 2D.</p></li>
<li><p>compute the error function at each combination of <span class="math inline">\(c\)</span> and <span class="math inline">\(m\)</span>.</p></li>
</ul>
</section>
<section id="contour-plot-of-error" class="slide level3">
<h3>Contour Plot of Error</h3>
<ul>
<li>We can now make a contour plot.</li>
</ul>
<object class="svgplot " align data="../slides/diagrams/ml/regression_contour.svg">
</object>
</section>
<section id="steepest-descent" class="slide level3">
<h3>Steepest Descent</h3>
<ul>
<li>Minimize the sum of squares error function.</li>
<li>One way of doing that is gradient descent.</li>
<li>Initialize with a guess for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span></li>
<li>update that guess by subtracting a portion of the gradient from the guess.</li>
<li>Like walking down a hill in the steepest direction of the hill to get to the bottom.</li>
</ul>
</section>
<section id="algorithm" class="slide level3">
<h3>Algorithm</h3>
<ul>
<li>We start with a guess for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>.</li>
</ul>
</section>
<section id="offset-gradient" class="slide level3">
<h3>Offset Gradient</h3>
<ul>
<li>Now we need to compute the gradient of the error function, firstly with respect to <span class="math inline">\(c\)</span>,</li>
</ul>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} c} =
-2\sum_{i=1}^\numData (\dataScalar_i - m\inputScalar_i - c)\]</span></p>
<ul>
<li>This is computed in python as follows</li>
</ul>
</section>
<section id="deriving-the-gradient" class="slide level3">
<h3>Deriving the Gradient</h3>
<p>To see how the gradient was derived, first note that the <span class="math inline">\(c\)</span> appears in every term in the sum. So we are just differentiating <span class="math inline">\((\dataScalar_i - m\inputScalar_i - c)^2\)</span> for each term in the sum. The gradient of this term with respect to <span class="math inline">\(c\)</span> is simply the gradient of the outer quadratic, multiplied by the gradient with respect to <span class="math inline">\(c\)</span> of the part inside the quadratic. The gradient of a quadratic is two times the argument of the quadratic, and the gradient of the inside linear term is just minus one. This is true for all terms in the sum, so we are left with the sum in the gradient.</p>
</section>
<section id="slope-gradient" class="slide level3">
<h3>Slope Gradient</h3>
<p>The gradient with respect tom <span class="math inline">\(m\)</span> is similar, but now the gradient of the quadratic’s argument is <span class="math inline">\(-\inputScalar_i\)</span> so the gradient with respect to <span class="math inline">\(m\)</span> is</p>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} m} = -2\sum_{i=1}^\numData \inputScalar_i(\dataScalar_i - m\inputScalar_i -
c)\]</span></p>
<p>which can be implemented in python (numpy) as</p>
</section>
<section id="update-equations" class="slide level3">
<h3>Update Equations</h3>
<ul>
<li>Now we have gradients with respect to <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>.</li>
<li>Can update our inital guesses for <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> using the gradient.</li>
<li>We don’t want to just subtract the gradient from <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>,</li>
<li>We need to take a <em>small</em> step in the gradient direction.</li>
<li>Otherwise we might overshoot the minimum.</li>
<li>We want to follow the gradient to get to the minimum, the gradient changes all the time.</li>
</ul>
</section>
<section id="move-in-direction-of-gradient" class="slide level3">
<h3>Move in Direction of Gradient</h3>
<object class="svgplot " align data="../slides/diagrams/ml/regression_contour_step001.svg">
</object>
</section>
<section id="update-equations-1" class="slide level3">
<h3>Update Equations</h3>
<ul>
<li><p>The step size has already been introduced, it’s again known as the learning rate and is denoted by <span class="math inline">\(\learnRate\)</span>. <span class="math display">\[
  c_\text{new}\leftarrow c_{\text{old}} - \learnRate \frac{\text{d}\errorFunction(m, c)}{\text{d}c}
\]</span></p></li>
<li>gives us an update for our estimate of <span class="math inline">\(c\)</span> (which in the code we’ve been calling <code>c_star</code> to represent a common way of writing a parameter estimate, <span class="math inline">\(c^*\)</span>) and <span class="math display">\[
m_\text{new} \leftarrow m_{\text{old}} - \learnRate \frac{\text{d}\errorFunction(m, c)}{\text{d}m}
\]</span></li>
<li><p>Giving us an update for <span class="math inline">\(m\)</span>.</p></li>
</ul>
</section>
<section id="update-code" class="slide level3">
<h3>Update Code</h3>
<ul>
<li>These updates can be coded as</li>
</ul>
<!-- SECTION Iterating Updates -->
<ul>
<li>Fit model by descending gradient.</li>
</ul>
</section>
<section id="gradient-descent-algorithm" class="slide level3">
<h3>Gradient Descent Algorithm</h3>
<script>
showDivs(1, 'regression_contour_fit');
</script>
<button onclick="plusDivs(-1, 'regression_contour_fit')">
❮
</button>
<button onclick="plusDivs(1, 'regression_contour_fit')">
❯
</button>
<p><input id="range-regression_contour_fit" type="range" min="1" max="28" value="1" onchange="setDivs('regression_contour_fit')" oninput="setDivs('regression_contour_fit')"> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit000.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit001.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit002.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit003.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit004.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit005.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit006.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit007.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit008.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit009.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit010.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit011.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit012.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit013.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit014.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit015.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit016.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit017.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit018.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit019.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit020.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit021.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit022.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit023.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit024.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit025.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit026.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit027.svg"></object> <object class="svgplot regression_contour_fit" align="" data="../slides/diagrams/ml/regression_contour_fit028.svg"></object></p>
</section>
<section id="stochastic-gradient-descent" class="slide level3">
<h3>Stochastic Gradient Descent</h3>
<ul>
<li>If <span class="math inline">\(\numData\)</span> is small, gradient descent is fine.</li>
<li>But sometimes (e.g. on the internet <span class="math inline">\(\numData\)</span> could be a billion.</li>
<li>Stochastic gradient descent is more similar to perceptron.</li>
<li>Look at gradient of one data point at a time rather than summing across <em>all</em> data points)</li>
<li>This gives a stochastic estimate of gradient.</li>
</ul>
</section>
<section id="stochastic-gradient-descent-1" class="slide level3">
<h3>Stochastic Gradient Descent</h3>
<ul>
<li>The real gradient with respect to <span class="math inline">\(m\)</span> is given by</li>
</ul>
<p><span class="math display">\[\frac{\text{d}\errorFunction(m, c)}{\text{d} m} = -2\sum_{i=1}^\numData \inputScalar_i(\dataScalar_i -
m\inputScalar_i - c)\]</span></p>
<p>but it has <span class="math inline">\(\numData\)</span> terms in the sum. Substituting in the gradient we can see that the full update is of the form</p>
<p><span class="math display">\[m_\text{new} \leftarrow
m_\text{old} + 2\learnRate \left[\inputScalar_1 (\dataScalar_1 - m_\text{old}\inputScalar_1 - c_\text{old}) + (\inputScalar_2 (\dataScalar_2 -   m_\text{old}\inputScalar_2 - c_\text{old}) + \dots + (\inputScalar_n (\dataScalar_n - m_\text{old}\inputScalar_n - c_\text{old})\right]\]</span></p>
<p>This could be split up into lots of individual updates <span class="math display">\[m_1 \leftarrow m_\text{old} + 2\learnRate \left[\inputScalar_1 (\dataScalar_1 - m_\text{old}\inputScalar_1 -
c_\text{old})\right]\]</span> <span class="math display">\[m_2 \leftarrow m_1 + 2\learnRate \left[\inputScalar_2 (\dataScalar_2 -
m_\text{old}\inputScalar_2 - c_\text{old})\right]\]</span> <span class="math display">\[m_3 \leftarrow m_2 + 2\learnRate
\left[\dots\right]\]</span> <span class="math display">\[m_n \leftarrow m_{n-1} + 2\learnRate \left[\inputScalar_n (\dataScalar_n -
m_\text{old}\inputScalar_n - c_\text{old})\right]\]</span></p>
<p>which would lead to the same final update.</p>
</section>
<section id="updating-c-and-m" class="slide level3">
<h3>Updating <span class="math inline">\(c\)</span> and <span class="math inline">\(m\)</span></h3>
<ul>
<li>In the sum we don’t <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span> we use for computing the gradient term at each update.</li>
<li>In stochastic gradient descent we <em>do</em> change them.</li>
<li>This means it’s not quite the same as steepest desceint.</li>
<li>But we can present each data point in a random order, like we did for the perceptron.</li>
<li>This makes the algorithm suitable for large scale web use (recently this domain is know as ‘Big Data’) and algorithms like this are widely used by Google, Microsoft, Amazon, Twitter and Facebook.</li>
</ul>
</section>
<section id="stochastic-gradient-descent-2" class="slide level3">
<h3>Stochastic Gradient Descent</h3>
<ul>
<li>Or more accurate, since the data is normally presented in a random order we just can write <span class="math display">\[
  m_\text{new} = m_\text{old} + 2\learnRate\left[\inputScalar_i (\dataScalar_i - m_\text{old}\inputScalar_i - c_\text{old})\right]
  \]</span></li>
</ul>
</section>
<section id="sgd-for-linear-regression" class="slide level3">
<h3>SGD for Linear Regression</h3>
<p>Putting it all together in an algorithm, we can do stochastic gradient descent for our regression data.</p>
<script>
showDivs(0, 'regression_sgd_contour_fit');
</script>
<button onclick="plusDivs(-1, 'regression_sgd_contour_fit')">
❮
</button>
<button onclick="plusDivs(1, 'regression_sgd_contour_fit')">
❯
</button>
<p><input id="range-regression_sgd_contour_fit" type="range" min="0" max="58" value="0" onchange="setDivs('regression_sgd_contour_fit')" oninput="setDivs('regression_sgd_contour_fit')"> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit000.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit001.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit002.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit003.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit004.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit005.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit006.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit007.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit008.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit009.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit010.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit011.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit012.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit013.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit014.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit015.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit016.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit017.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit018.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit019.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit020.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit021.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit022.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit023.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit024.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit025.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit026.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit027.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit028.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit029.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit030.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit031.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit032.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit033.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit034.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit035.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit036.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit037.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit038.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit039.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit040.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit041.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit042.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit043.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit044.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit045.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit046.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit047.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit048.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit049.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit050.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit051.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit052.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit053.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit054.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit055.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit056.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit057.svg"></object> <object class="svgplot regression_sgd_contour_fit" align="" data="../slides/diagrams/ml/regression_sgd_contour_fit058.svg"></object></p>
</section>
<section id="reflection-on-linear-regression-and-supervised-learning" class="slide level3">
<h3>Reflection on Linear Regression and Supervised Learning</h3>
<p>Think about:</p>
<ol type="1">
<li><p>What effect does the learning rate have in the optimization? What’s the effect of making it too small, what’s the effect of making it too big? Do you get the same result for both stochastic and steepest gradient descent?</p></li>
<li><p>The stochastic gradient descent doesn’t help very much for such a small data set. It’s real advantage comes when there are many, you’ll see this in the lab.</p></li>
</ol>
</section>
<section id="log-likelihood-for-multivariate-regression" class="slide level3">
<h3>Log Likelihood for Multivariate Regression</h3>
<p>The likelihood of a single data point is</p>
<div class="fragment">
<p><span class="math display">\[p\left(\dataScalar_i|\inputScalar_i\right)=\frac{1}{\sqrt{2\pi\dataStd^2}}\exp\left(-\frac{\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}\right).\]</span></p>
</div>
<div class="fragment">
<p>Leading to a log likelihood for the data set of</p>
</div>
<div class="fragment">
<p><span class="math display">\[L(\mappingVector,\dataStd^2)= -\frac{\numData}{2}\log \dataStd^2-\frac{\numData}{2}\log 2\pi -\frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}.\]</span></p>
</div>
</section>
<section id="error-function-1" class="slide level3">
<h3>Error Function</h3>
<p>And a corresponding error function of <span class="math display">\[\errorFunction(\mappingVector,\dataStd^2)=\frac{\numData}{2}\log\dataStd^2 + \frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\mappingVector^{\top}\inputVector_i\right)^{2}}{2\dataStd^2}.\]</span></p>
</section>
<section id="expand-the-brackets" class="slide level3">
<h3>Expand the Brackets</h3>
<p><span class="math display">\[
\begin{align*}
  \errorFunction(\mappingVector,\dataStd^2)  = &amp;
\frac{\numData}{2}\log \dataStd^2 + \frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i^{2}-\frac{1}{\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i\mappingVector^{\top}\inputVector_i\\&amp;+\frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\mappingVector^{\top}\inputVector_i\inputVector_i^{\top}\mappingVector
+\text{const}.\\
    = &amp; \frac{\numData}{2}\log \dataStd^2 + \frac{1}{2\dataStd^2}\sum
_{i=1}^{\numData}\dataScalar_i^{2}-\frac{1}{\dataStd^2}
\mappingVector^\top\sum_{i=1}^{\numData}\inputVector_i\dataScalar_i\\&amp;+\frac{1}{2\dataStd^2}
\mappingVector^{\top}\left[\sum
_{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]\mappingVector +\text{const}.
\end{align*}
\]</span></p>
<!-- SECTION Multiple Input Solution with Linear Algebra -->
<!-- SECTION Design Matrix -->
<!-- SECTION Objective Optimisation -->
</section>
<section id="multivariate-derivatives" class="slide level3">
<h3>Multivariate Derivatives</h3>
<ul>
<li>We will need some multivariate calculus.</li>
<li>For now some simple multivariate differentiation: <span class="math display">\[\frac{\text{d}{\mathbf{a}^{\top}}{\mappingVector}}{\text{d}\mappingVector}=\mathbf{a}\]</span> and <span class="math display">\[\frac{\mappingVector^{\top}\mathbf{A}\mappingVector}{\text{d}\mappingVector}=\left(\mathbf{A}+\mathbf{A}^{\top}\right)\mappingVector\]</span> or if <span class="math inline">\(\mathbf{A}\)</span> is symmetric (<em>i.e.</em> <span class="math inline">\(\mathbf{A}=\mathbf{A}^{\top}\)</span>) <span class="math display">\[\frac{\text{d}\mappingVector^{\top}\mathbf{A}\mappingVector}{\text{d}\mappingVector}=2\mathbf{A}\mappingVector.\]</span></li>
</ul>
</section>
<section id="differentiate-the-objective" class="slide level3">
<h3>Differentiate the Objective</h3>
<p><span align="left">Differentiating with respect to the vector <span class="math inline">\(\mappingVector\)</span> we obtain</span> <span class="math display">\[
\frac{\partial L\left(\mappingVector,\dataStd^2 \right)}{\partial
\mappingVector}=\frac{1}{\dataStd^2} \sum _{i=1}^{\numData}\inputVector_i \dataScalar_i-\frac{1}{\dataStd^2}
\left[\sum _{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]\mappingVector
\]</span> Leading to <span class="math display">\[
\mappingVector^{*}=\left[\sum
_{i=1}^{\numData}\inputVector_i\inputVector_i^{\top}\right]^{-1}\sum
_{i=1}^{\numData}\inputVector_i\dataScalar_i,
\]</span></p>
</section>
<section id="differentiate-the-objective-1" class="slide level3">
<h3>Differentiate the Objective</h3>
<p>Rewrite in matrix notation: <span class="math display">\[
\sum_{i=1}^{\numData}\inputVector_i\inputVector_i^\top = \inputMatrix^\top \inputMatrix
\]</span> <span class="math display">\[
\sum_{i=1}^{\numData}\inputVector_i\dataScalar_i = \inputMatrix^\top \dataVector
\]</span></p>
<!-- SECTION Update Equation for Global Optimum -->
</section>
<section id="update-equations-2" class="slide level3">
<h3>Update Equations</h3>
<ul>
<li>Update for <span class="math inline">\(\mappingVector^{*}\)</span>. <span class="math display">\[\mappingVector^{*} = \left(\inputMatrix^\top \inputMatrix\right)^{-1} \inputMatrix^\top \dataVector\]</span></li>
<li>The equation for <span class="math inline">\(\left.\dataStd^2\right.^{*}\)</span> may also be found <span class="math display">\[\left.\dataStd^2\right.^{{*}}=\frac{\sum_{i=1}^{\numData}\left(\dataScalar_i-\left.\mappingVector^{*}\right.^{\top}\inputVector_i\right)^{2}}{\numData}.\]</span></li>
</ul>
</section>
<section id="solving-the-multivariate-system" class="slide level3">
<h3>Solving the Multivariate System</h3>
</section>
<section id="reading-1" class="slide level3">
<h3>Reading</h3>
<ul>
<li>Section 1.3 of <span class="citation" data-cites="Rogers:book11">Rogers and Girolami (2011)</span> for Matrix &amp; Vector Review.</li>
</ul>
</section>
<section id="basis-functions" class="slide level3">
<h3>Basis Functions</h3>
</section>
<section id="quadratic-basis" class="slide level3">
<h3>Quadratic Basis</h3>
<ul>
<li>Basis functions can be global. E.g. quadratic basis: <span class="math display">\[
  \basisVector = [1, \inputScalar, \inputScalar^2]
  \]</span></li>
</ul>
<p><span class="math display">\[
\begin{align*}
\basisFunc_1(\inputScalar) = 1, \\
\basisFunc_2(\inputScalar) = x, \\
\basisFunc_3(\inputScalar) = \inputScalar^2.
\end{align*}
\]</span></p>
<p><span class="math display">\[
\basisVector(\inputScalar) = \begin{bmatrix} 1\\ x \\ \inputScalar^2\end{bmatrix}.
\]</span></p>
</section>
<section id="matrix-valued-function" class="slide level3">
<h3>Matrix Valued Function</h3>
<p><span class="math display">\[
\basisMatrix(\inputVector) = 
\begin{bmatrix} 1 &amp; \inputScalar_1 &amp;
\inputScalar_1^2 \\
1 &amp; \inputScalar_2 &amp; \inputScalar_2^2\\
\vdots &amp; \vdots &amp; \vdots \\
1 &amp; \inputScalar_n &amp; \inputScalar_n^2
\end{bmatrix}
\]</span></p>
</section>
<section id="functions-derived-from-quadratic-basis" class="slide level3">
<h3>Functions Derived from Quadratic Basis</h3>
<p><span class="math display">\[
\mappingFunction(\inputScalar) = {\color{cyan}\mappingScalar_0}   + {\color{green}\mappingScalar_1 \inputScalar} + {\color{yellow}\mappingScalar_2 \inputScalar^2}
\]</span></p>
<script>
showDivs(0, 'quadratic_basis');
</script>
<button onclick="plusDivs(-1, 'quadratic_basis')">
❮
</button>
<button onclick="plusDivs(1, 'quadratic_basis')">
❯
</button>
<p><input id="range-quadratic_basis" type="range" min="0" max="2" value="0" onchange="setDivs('quadratic_basis')" oninput="setDivs('quadratic_basis')"> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis000.svg"></object> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis001.svg"></object> <object class="svgplot quadratic_basis" align="" data="../slides/diagrams/ml/quadratic_basis002.svg"></object></p>
</section>
<section id="quadratic-functions" class="slide level3">
<h3>Quadratic Functions</h3>
<script>
showDivs(0, 'quadratic_function');
</script>
<button onclick="plusDivs(-1, 'quadratic_function')">
❮
</button>
<button onclick="plusDivs(1, 'quadratic_function')">
❯
</button>
<p><input id="range-quadratic_function" type="range" min="0" max="2" value="0" onchange="setDivs('quadratic_function')" oninput="setDivs('quadratic_function')"> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function000.svg"></object> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function001.svg"></object> <object class="svgplot quadratic_function" align="" data="../slides/diagrams/ml/quadratic_function002.svg"></object></p>
</section>
<section id="polynomial-fits-to-olympic-data" class="slide level3">
<h3>Polynomial Fits to Olympic Data</h3>
<script>
showDivs(1, 'olympic_LM_polynomial_num_basis');
</script>
<button onclick="plusDivs(-1, 'olympic_LM_polynomial_num_basis')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_LM_polynomial_num_basis')">
❯
</button>
<p><input id="range-olympic_LM_polynomial_num_basis" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_LM_polynomial_num_basis')" oninput="setDivs('olympic_LM_polynomial_num_basis')"> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis001.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis002.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis003.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis004.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis005.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis006.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis007.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis008.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis009.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis010.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis011.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis012.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis013.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis014.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis015.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis016.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis017.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis018.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis019.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis020.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis021.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis022.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis023.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis024.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis025.svg"></object> <object class="svgplot olympic_LM_polynomial_num_basis" align="" data="../slides/diagrams/ml/olympic_LM_polynomial_num_basis026.svg"></object></p>
<!-- SECTION Underdetermined System -->
</section>
<section id="underdetermined-system" class="slide level3">
<h3>Underdetermined System</h3>
<ul>
<li>What about two unknowns and <em>one</em> observation? <span class="math display">\[\dataScalar_1 =  m\inputScalar_1 + c\]</span></li>
</ul>
<div class="fragment">
<p>Can compute <span class="math inline">\(m\)</span> given <span class="math inline">\(c\)</span>. <span class="math display">\[m = \frac{\dataScalar_1 - c}{\inputScalar}\]</span></p>
</div>
</section>
<section id="underdetermined-system-1" class="slide level3">
<h3>Underdetermined System</h3>
<script>
showDivs(1, 'under_determined_system');
</script>
<button onclick="plusDivs(-1, 'under_determined_system')">
❮
</button>
<button onclick="plusDivs(1, 'under_determined_system')">
❯
</button>
<p><input id="range-under_determined_system" type="range" min="1" max="3" value="1" onchange="setDivs('under_determined_system')" oninput="setDivs('under_determined_system')"> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system000.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system001.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system002.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system003.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system004.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system005.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system006.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system007.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system008.svg"></object> <object class="svgplot under_determined_system" align="" data="../slides/diagrams/ml/under_determined_system009.svg"></object></p>
</section>
<section id="alan-turing" class="slide level3">
<h3>Alan Turing</h3>
<table>
<tr>
<td width="50%">
<img class="" src="../slides/diagrams/turing-times.gif" width="" align="center" style="background:none; border:none; box-shadow:none;">
</td>
<td width="50%">
<img class="" src="../slides/diagrams/turing-run.jpg" width="" align="center" style="background:none; border:none; box-shadow:none;">
</td>
</tr>
</table>
<center>
<em>Alan Turing, in 1946 he was only 11 minutes slower than the winner of the 1948 games. Would he have won a hypothetical games held in 1946? Source: <a href="http://www.turing.org.uk/scrapbook/run.html">Alan Turing Internet Scrapbook</a>.</em>
</center>
</section>
<section id="probability-winning-olympics" class="slide level3">
<h3>Probability Winning Olympics?</h3>
<ul>
<li>He was a formidable Marathon runner.</li>
<li>In 1946 he ran a time 2 hours 46 minutes.
<ul>
<li>That’s a pace of 3.95 min/km.</li>
</ul></li>
<li>What is the probability he would have won an Olympics if one had been held in 1946?</li>
</ul>
</section>
<section id="prior-distribution" class="slide level3">
<h3>Prior Distribution</h3>
<ul>
<li><p>Bayesian inference requires a prior on the parameters.</p></li>
<li><p>The prior represents your belief <em>before</em> you see the data of the likely value of the parameters.</p></li>
<li><p>For linear regression, consider a Gaussian prior on the intercept: <span class="math display">\[c \sim \gaussianSamp{0}{\alpha_1}\]</span></p></li>
</ul>
</section>
<section id="posterior-distribution" class="slide level3">
<h3>Posterior Distribution</h3>
<ul>
<li>Posterior distribution is found by combining the prior with the likelihood.</li>
<li>Posterior distribution is your belief <em>after</em> you see the data of the likely value of the parameters.</li>
<li>The posterior is found through <strong>Bayes’ Rule</strong> <span class="math display">\[
  p(c|\dataScalar) = \frac{p(\dataScalar|c)p(c)}{p(\dataScalar)}
  \]</span></li>
</ul>
</section>
<section id="bayes-update" class="slide level3">
<h3>Bayes Update</h3>
<script>
showDivs(1, 'dem_gaussian');
</script>
<button onclick="plusDivs(-1, 'dem_gaussian')">
❮
</button>
<button onclick="plusDivs(1, 'dem_gaussian')">
❯
</button>
<p><input id="range-dem_gaussian" type="range" min="1" max="3" value="1" onchange="setDivs('dem_gaussian')" oninput="setDivs('dem_gaussian')"> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian001.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian002.svg"></object> <object class="svgplot dem_gaussian" align="" data="../slides/diagrams/ml/dem_gaussian003.svg"></object></p>
</section>
<section id="stages-to-derivation-of-the-posterior" class="slide level3">
<h3>Stages to Derivation of the Posterior</h3>
<ul>
<li>Multiply likelihood by prior</li>
<li>they are “exponentiated quadratics”, the answer is always also an exponentiated quadratic because <span class="math inline">\(\exp(a^2)\exp(b^2) = \exp(a^2 + b^2)\)</span>.</li>
<li>Complete the square to get the resulting density in the form of a Gaussian.</li>
<li>Recognise the mean and (co)variance of the Gaussian. This is the estimate of the posterior.</li>
</ul>
</section>
<section id="main-trick" class="slide level3">
<h3>Main Trick</h3>
<p><span class="math display">\[p(c) = \frac{1}{\sqrt{2\pi\alpha_1}} \exp\left(-\frac{1}{2\alpha_1}c^2\right)\]</span> <span class="math display">\[p(\dataVector|\inputVector, c, m, \dataStd^2) = \frac{1}{\left(2\pi\dataStd^2\right)^{\frac{\numData}{2}}} \exp\left(-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i - m\inputScalar_i - c)^2\right)\]</span></p>
</section>
<section id="section-5" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) = \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{p(\dataVector|\inputVector, m, \dataStd^2)}\]</span></p>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) =  \frac{p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)}{\int p(\dataVector|\inputVector, c, m, \dataStd^2)p(c) \text{d} c}\]</span></p>
</section>
<section id="section-6" class="slide level3">
<h3></h3>
<p><span class="math display">\[p(c| \dataVector, \inputVector, m, \dataStd^2) \propto  p(\dataVector|\inputVector, c, m, \dataStd^2)p(c)\]</span></p>
<p><span class="math display">\[\begin{aligned}
    \log p(c | \dataVector, \inputVector, m, \dataStd^2) =&amp;-\frac{1}{2\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-c - m\inputScalar_i)^2-\frac{1}{2\alpha_1} c^2 + \text{const}\\
     = &amp;-\frac{1}{2\dataStd^2}\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)^2 -\left(\frac{\numData}{2\dataStd^2} + \frac{1}{2\alpha_1}\right)c^2\\
    &amp; + c\frac{\sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)}{\dataStd^2},
  \end{aligned}\]</span></p>
</section>
<section id="section-7" class="slide level3">
<h3></h3>
<p>complete the square of the quadratic form to obtain <span class="math display">\[\log p(c | \dataVector, \inputVector, m, \dataStd^2) = -\frac{1}{2\tau^2}(c - \mu)^2 +\text{const},\]</span> where <span class="math inline">\(\tau^2 = \left(\numData\dataStd^{-2} +\alpha_1^{-1}\right)^{-1}\)</span> and <span class="math inline">\(\mu = \frac{\tau^2}{\dataStd^2} \sum_{i=1}^\numData(\dataScalar_i-m\inputScalar_i)\)</span>.</p>
</section>
<section id="two-dimensional-gaussian" class="slide level3">
<h3>Two Dimensional Gaussian</h3>
<ul>
<li>Consider height, <span class="math inline">\(h/m\)</span> and weight, <span class="math inline">\(w/kg\)</span>.</li>
<li>Could sample height from a distribution: <span class="math display">\[
  p(h) \sim \gaussianSamp{1.7}{0.0225}
  \]</span></li>
<li>And similarly weight: <span class="math display">\[
  p(w) \sim \gaussianSamp{75}{36}
  \]</span></li>
</ul>
</section>
<section id="height-and-weight-models" class="slide level3">
<h3>Height and Weight Models</h3>
<object class="svgplot " align data="../slides/diagrams/ml/height_weight_gaussian.svg">
</object>
<p>Gaussian distributions for height and weight.</p>
</section>
<section id="sampling-two-dimensional-variables" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'independent_height_weight');
</script>
<button onclick="plusDivs(-1, 'independent_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'independent_height_weight')">
❯
</button>
<p><input id="range-independent_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('independent_height_weight')" oninput="setDivs('independent_height_weight')"> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight000.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight001.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight002.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight003.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight004.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight005.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight006.svg"></object> <object class="svgplot independent_height_weight" align="" data="../slides/diagrams/ml/independent_height_weight007.svg"></object></p>
</section>
<section id="independence-assumption" class="slide level3">
<h3>Independence Assumption</h3>
<ul>
<li><p>This assumes height and weight are independent. <span class="math display">\[p(h, w) = p(h)p(w)\]</span></p></li>
<li><p>In reality they are dependent (body mass index) <span class="math inline">\(= \frac{w}{h^2}\)</span>.</p></li>
</ul>
</section>
<section id="sampling-two-dimensional-variables-1" class="slide level3">
<h3>Sampling Two Dimensional Variables</h3>
<script>
showDivs(0, 'correlated_height_weight');
</script>
<button onclick="plusDivs(-1, 'correlated_height_weight')">
❮
</button>
<button onclick="plusDivs(1, 'correlated_height_weight')">
❯
</button>
<p><input id="range-correlated_height_weight" type="range" min="0" max="7" value="0" onchange="setDivs('correlated_height_weight')" oninput="setDivs('correlated_height_weight')"> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight000.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight001.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight002.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight003.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight004.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight005.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight006.svg"></object> <object class="svgplot correlated_height_weight" align="" data="../slides/diagrams/ml/correlated_height_weight007.svg"></object></p>
</section>
<section id="independent-gaussians" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = p(w)p(h)
\]</span></p>
</section>
<section id="independent-gaussians-1" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi \dataStd_1^2}\sqrt{2\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\frac{(w-\meanScalar_1)^2}{\dataStd_1^2} + \frac{(h-\meanScalar_2)^2}{\dataStd_2^2}\right)\right)
\]</span></p>
</section>
<section id="independent-gaussians-2" class="slide level3">
<h3>Independent Gaussians</h3>
<p><small> <span class="math display">\[
p(w, h) = \frac{1}{\sqrt{2\pi\dataStd_1^22\pi\dataStd_2^2}} \exp\left(-\frac{1}{2}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)^\top\begin{bmatrix}\dataStd_1^2&amp; 0\\0&amp;\dataStd_2^2\end{bmatrix}^{-1}\left(\begin{bmatrix}w \\ h\end{bmatrix} - \begin{bmatrix}\meanScalar_1 \\ \meanScalar_2\end{bmatrix}\right)\right)
\]</span> </small></p>
</section>
<section id="independent-gaussians-3" class="slide level3">
<h3>Independent Gaussians</h3>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi \mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\mathbf{D}^{-1}(\dataVector - \meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-1" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)^\top\mathbf{D}^{-1}(\rotationMatrix^\top\dataVector - \rotationMatrix^\top\meanVector)\right)
\]</span></p>
</section>
<section id="correlated-gaussian-2" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\mathbf{D}}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\rotationMatrix\mathbf{D}^{-1}\rotationMatrix^\top(\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix^{-1} = \rotationMatrix \mathbf{D}^{-1} \rotationMatrix^\top
\]</span></p>
</section>
<section id="correlated-gaussian-3" class="slide level3">
<h3>Correlated Gaussian</h3>
<p>Form correlated from original by rotating the data space using matrix <span class="math inline">\(\rotationMatrix\)</span>.</p>
<p><span class="math display">\[
p(\dataVector) = \frac{1}{\det{2\pi\covarianceMatrix}^{\frac{1}{2}}} \exp\left(-\frac{1}{2}(\dataVector - \meanVector)^\top\covarianceMatrix^{-1} (\dataVector - \meanVector)\right)
\]</span> this gives a covariance matrix: <span class="math display">\[
\covarianceMatrix = \rotationMatrix \mathbf{D} \rotationMatrix^\top
\]</span></p>
</section>
<section id="sampling-the-prior" class="slide level3">
<h3>Sampling the Prior</h3>
<ul>
<li>Always useful to perform a ‘sanity check’ and sample from the prior before observing the data.</li>
<li>Since <span class="math inline">\(\dataVector = \basisMatrix \mappingVector + \noiseVector\)</span> just need to sample <span class="math display">\[
  \mappingVector \sim \gaussianSamp{0}{\alpha\eye}
  \]</span> <span class="math display">\[
  \noiseVector \sim \gaussianSamp{\zerosVector}{\dataStd^2}
  \]</span> with <span class="math inline">\(\alpha=1\)</span> and <span class="math inline">\(\dataStd^2 = 0.01\)</span>.</li>
</ul>
</section>
<section id="computing-the-posterior" class="slide level3">
<h3>Computing the Posterior</h3>
<p><span class="math display">\[
p(\mappingVector | \dataVector, \inputVector, \dataStd^2) = \gaussianDist{\mappingVector}{\meanVector_\mappingScalar}{\covarianceMatrix_\mappingScalar}
\]</span> with <span class="math display">\[
\covarianceMatrix_\mappingScalar = \left(\dataStd^{-2}\basisMatrix^\top \basisMatrix + \alpha^{-1}\eye\right)^{-1}
\]</span> and <span class="math display">\[
\meanVector_\mappingScalar = \covarianceMatrix_\mappingScalar \dataStd^{-2}\basisMatrix^\top \dataVector
\]</span></p>
</section>
<section id="olympic-data-with-bayesian-polynomials" class="slide level3">
<h3>Olympic Data with Bayesian Polynomials</h3>
<script>
showDivs(1, 'olympic_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_BLM_polynomial_number')" oninput="setDivs('olympic_BLM_polynomial_number')"> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="hold-out-validation" class="slide level3">
<h3>Hold Out Validation</h3>
<script>
showDivs(1, 'olympic_val_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_val_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_val_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_val_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_val_BLM_polynomial_number')" oninput="setDivs('olympic_val_BLM_polynomial_number')"> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_val_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_val_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="fold-cross-validation" class="slide level3">
<h3>5-fold Cross Validation</h3>
<script>
showDivs(1, 'olympic_5cv05_BLM_polynomial_number');
</script>
<button onclick="plusDivs(-1, 'olympic_5cv05_BLM_polynomial_number')">
❮
</button>
<button onclick="plusDivs(1, 'olympic_5cv05_BLM_polynomial_number')">
❯
</button>
<p><input id="range-olympic_5cv05_BLM_polynomial_number" type="range" min="1" max="26" value="1" onchange="setDivs('olympic_5cv05_BLM_polynomial_number')" oninput="setDivs('olympic_5cv05_BLM_polynomial_number')"> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number001.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number002.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number003.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number004.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number005.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number006.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number007.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number008.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number009.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number010.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number011.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number012.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number013.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number014.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number015.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number016.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number017.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number018.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number019.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number020.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number021.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number022.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number023.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number024.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number025.svg"></object> <object class="svgplot olympic_5cv05_BLM_polynomial_number" align="" data="../slides/diagrams/ml/olympic_5cv05_BLM_polynomial_number026.svg"></object></p>
</section>
<section id="marginal-likelihood" class="slide level3">
<h3>Marginal Likelihood</h3>
<ul>
<li><p>The marginal likelihood can also be computed, it has the form: <span class="math display">\[
  p(\dataVector|\inputMatrix, \dataStd^2, \alpha) = \frac{1}{(2\pi)^\frac{n}{2}\left|\kernelMatrix\right|^\frac{1}{2}} \exp\left(-\frac{1}{2} \dataVector^\top \kernelMatrix^{-1} \dataVector\right)
  \]</span> where <span class="math inline">\(\kernelMatrix = \alpha \basisMatrix\basisMatrix^\top + \dataStd^2 \eye\)</span>.</p></li>
<li><p>So it is a zero mean <span class="math inline">\(\numData\)</span>-dimensional Gaussian with covariance matrix <span class="math inline">\(\kernelMatrix\)</span>.</p></li>
</ul>
</section>
<section id="references" class="slide level3 unnumbered">
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Bishop:book06">
<p>Bishop, C.M., 2006. Pattern recognition and machine learning. springer.</p>
</div>
<div id="ref-Rogers:book11">
<p>Rogers, S., Girolami, M., 2011. A first course in machine learning. CRC Press.</p>
</div>
</div>
</section>
    </div>
  </div>

  <script src="reveal.js/lib/js/head.min.js"></script>
  <script src="reveal.js/js/reveal.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        // Push each slide change to the browser history
        history: true,
        // Transition style
        transition: 'None', // none/fade/slide/convex/concave/zoom

        // Optional reveal.js plugins
        dependencies: [
          { src: 'reveal.js/lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'reveal.js/plugin/zoom-js/zoom.js', async: true },
          { src: 'reveal.js/plugin/notes/notes.js', async: true }
        ]
      });
    </script>
    </body>
</html>
