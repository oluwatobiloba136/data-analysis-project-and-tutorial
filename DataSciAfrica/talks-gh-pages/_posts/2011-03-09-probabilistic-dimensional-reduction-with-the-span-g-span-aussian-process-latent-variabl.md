---
abstract: Density modelling in high dimensions is a very difficult problem. Traditional
  approaches, such as mixtures of Gaussians, typically fail to capture the structure
  of data sets in high dimensional spaces. In this talk we will argue that for many
  data sets of interest, the data can be represented as a lower dimensional manifold
  immersed in the higher dimensional space. We will then present the Gaussian Process
  Latent Variable Model (GP-LVM), a non-linear probabilistic variant of principal
  component analysis (PCA) which implicitly assumes that the data lies on a lower
  dimensional space. Having introduced the GP-LVM we will review extensions to the
  algorithm. Given time we will review dynamical extensions, Bayesian approaches to
  dimensionality determination, learning of large data sets. We will demonstrate the
  application of the model and its extensions to a range of data sets, including human
  motion data, speech data and video.
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Sheffield
  twitter: lawrennd
  url: http://inverseprobability.com
categories:
- Lawrence-loughborough11
day: 09
errata: []
extras:
- label: Bayesian GPLVM Software
  link: https://github.com/SheffieldML/vargplvm/
- label: Main Software
  link: https://github.com/SheffieldML/GPmat/
group: gplvm
key: Lawrence-loughborough11
layout: talk
linkpdf: ftp://ftp.dcs.shef.ac.uk/home/neil/loughborough_gplvm.pdf
month: 3
published: 2011-03-09
section: pre
title: Probabilistic Dimensional Reduction with the <span>G</span>aussian Process
  Latent Variable Model
venue: Department of Computer Science, University of Loughgborough, U.K.
year: '2011'
---