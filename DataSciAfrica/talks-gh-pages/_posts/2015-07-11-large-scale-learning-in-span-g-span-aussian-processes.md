---
abstract: "Gaussian process models view the kernel matrix as representing the covariance
  between data points. In a Gaussian process, the RKHS function is a mean of a posterior
  distribution over possible functions. Gaussian processes sustain uncertainty around
  this means and this leads to a posterior \\*covariance\\* function (or kernel) associated
  with the process. A complication for large scale Gaussian process models is the
  need to sustain the estimate for this covariance function. In this talk we\u2019ll
  review how this can be done probabilistically through a variational approach we
  know as \u2019variational compression\u2019."
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Sheffield
  twitter: lawrennd
  url: http://inverseprobability.com
categories:
- Lawrence-largeicml15
day: '11'
errata: []
extras: []
key: Lawrence-largeicml15
layout: talk
linkpdf: http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/parametric_icmllskw15.pdf
month: 7
published: 2015-07-11
section: pre
title: Large Scale Learning in <span>G</span>aussian Processes
venue: Large-Scale Kernel Learning Workshop @ICML2015
year: '2015'
---