---
abstract: "Supervised deep learning techniques now dominate in terms of performance
  for complex classification tasks such as ImageNet. For these, the set of inputs
  (features) and targets (labels) are typically well defined in advance. However,
  for many tasks in artificial intelligence the questions that need to be answered
  evolve, alongside the features that we can acquire. For example, imagine we wish
  to infer the health status of individuals by building population scale models based
  on clinical data. For most people in the population most of the data will be missing
  because clinical tests are not applied to patients as a matter of course. Indeed,
  some of the features we may wish to use in our model may not even exist when our
  model is first designed (e.g. emerging clinical tests and treatments). We refer
  to this scenario as \u2019massively missing data\u2019. It is a scenario humans
  are faced with every day. Almost all of the time we are missing almost all of the
  data. And yet we have no difficulty assimilating disparate pieces of information
  from a wide range of sources to draw inferences about our world. Implementing machine
  learning systems that can replicate this characteristic requires model architectures
  that can be adapted at \u2019runtime\u2019 as the data evolves, we don\u2019t want
  to be limited by decisions made at \u2019design time\u2019 when perhaps a more limited
  feature set existed. This poses particular challenges that we will address in this
  talk."
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Sheffield
  twitter: lawrennd
  url: http://inverseprobability.com
categories:
- Lawrence-facebook14
day: '20'
errata: []
extras: []
key: Lawrence-facebook14
layout: talk
linkpdf: http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/missing_facebook14.pdf
month: 3
published: 2014-03-20
section: pre
title: Modelling with Massively Missing Data
venue: Facebook, Menlo Park, CA
year: '2014'
---