---
title: "Fairness and Diversity of Decision Making"
venue: "Center for the Advanced Studies of the Behavioral Sciences, Joint American Academy and Royal Society Workshop"
abstract: "<p>Mathematical definitions of fairness insist on clearly categorized groups and clear mathematical interpretations of fairness. In law this arises through the concept of <em>unlawful</em> descrimination. There is no such thing as a correct model. We must accept that our predictions will sometimes be wrong. In the face of this certainty we have a choice: how we should be wrong. We can choose to be wrong by over-simplifying or we can choose to be wrong by over-complicating (given the available data). In machine learning this is known as the bias-variance dilemma. In this talk we consider the implications of the bias-variance dilemma for fairness of decision making.</p>"
author:
- given: Neil D.
  family: Lawrence
  url: http://inverseprobability.com
  institute: Amazon Cambridge and University of Sheffield
  twitter: lawrennd
  gscholar: r3SJcvoAAAAJ
  orchid: 
blog: 2017-11-15-decision-making.md
blog: 2018-02-06-natural-and-artificial-intelligence.md
blog: 2015-12-04-what-kind-of-ai.md
date: 2018-11-08
published: 2018-11-08
reveal: 2018-11-08-fairness-and-diversity-of-decision-making.slides.html
layout: talk
categories:
- notes
---


<div style="display:none">
  $${% include talk-notation.tex %}$$
</div>

<p><!--% not ipynb--></p>
<!--include{_philosophy/includes/utilitarianism.md}
include{_philosophy/includes/utility-utilitarianism.md}
include{_philosophy/includes/trolley-push.md}-->
<p>Machine learning allows us to extract knowledge from data to form a prediction.</p>
<p><span class="math display">\[ \text{data} + \text{model} \xrightarrow{\text{compute}} \text{prediction}\]</span></p>
<p>A machine learning prediction is made by combining a model with data to form the prediction. The manner in which this is done gives us the machine learning <em>algorithm</em>.</p>
<p>Machine learning models are <em>mathematical models</em> which make weak assumptions about data, e.g. smoothness assumptions. By combining these assumptions with the data we observe we can interpolate between data points or, occasionally, extrapolate into the future.</p>
<p>Machine learning is a technology which strongly overlaps with the methodology of statistics. From a historical/philosophical view point, machine learning differs from statistics in that the focus in the machine learning community has been primarily on accuracy of prediction, whereas the focus in statistics is typically on the interpretability of a model and/or validating a hypothesis through data collection.</p>
<p>The rapid increase in the availability of compute and data has led to the increased prominence of machine learning. This prominence is surfacing in two different, but overlapping domains: data science and artificial intelligence.</p>
<!--







-->
<!--





Helper function for sampling data from two different classes.

```{.python}
import numpy as np
```

```{.python}
def create_data(per_cluster=30):
    """Create a randomly sampled data set
    
    :param per_cluster: number of points in each cluster
    """
    X = []
    y = []
    scale = 3
    prec = 1/(scale*scale)
    pos_mean = [[-1, 0],[0,0.5],[1,0]]
    pos_cov = [[prec, 0.], [0., prec]]
    neg_mean = [[0, -0.5],[0,-0.5],[0,-0.5]]
    neg_cov = [[prec, 0.], [0., prec]]
    for mean in pos_mean:
        X.append(np.random.multivariate_normal(mean=mean, cov=pos_cov, size=per_class))
        y.append(np.ones((per_class, 1)))
    for mean in neg_mean:
        X.append(np.random.multivariate_normal(mean=mean, cov=neg_cov, size=per_class))
        y.append(np.zeros((per_class, 1)))
    return np.vstack(X), np.vstack(y).flatten()
```
        
Helper function for plotting the decision boundary of the SVM.

```{.python}
def plot_contours(ax, cl, xx, yy, **params):
    """Plot the decision boundaries for a classifier.

    :param ax: matplotlib axes object
    :param cl: a classifier
    :param xx: meshgrid ndarray
    :param yy: meshgrid ndarray
    :param params: dictionary of params to pass to contourf, optional
    """
    Z = cl.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    # Plot decision boundary and regions
    out = ax.contour(xx, yy, Z, 
                     levels=[-1., 0., 1], 
                     colors='black', 
                     linestyles=['dashed', 'solid', 'dashed'])
    out = ax.contourf(xx, yy, Z, 
                     levels=[Z.min(), 0, Z.max()], 
                     colors=[[0.5, 1.0, 0.5], [1.0, 0.5, 0.5]])
    return out
```



```{.python}
def decision_boundary_plot(models, X, y, axs, filename, titles, xlim, ylim):
    """Plot a decision boundary on the given axes
    
    :param axs: the axes to plot on.
    :param models: the SVM models to plot
    :param titles: the titles for each axis
    :param X: input training data
    :param y: target training data"""
    for ax in axs.flatten():
        ax.clear()
    X0, X1 = X[:, 0], X[:, 1]
    if xlim is None:
        xlim = [X0.min()-1, X0.max()+1]
    if ylim is None:
        ylim = [X1.min()-1, X1.max()+1]
    xx, yy = np.meshgrid(np.arange(xlim[0], xlim[1], 0.02),
                         np.arange(ylim[0], ylim[1], 0.02))
    for cl, title, ax in zip(models, titles, axs.flatten()):
        plot_contours(ax, cl, xx, yy,
                      cmap=plt.cm.coolwarm, alpha=0.8)
        ax.plot(X0[y==1], X1[y==1], 'r.', markersize=10)
        ax.plot(X0[y==0], X1[y==0], 'g.', markersize=10)
        ax.set_xlim(xlim)
        ax.set_ylim(ylim)
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(title)
        mlai.write_figure(os.path.join(filename),
                          figure=fig,
                          transparent=True)
    return xlim, ylim
```


```{.python}
import matplotlib
font = {'family' : 'sans',
        'weight' : 'bold',
        'size'   : 22}

matplotlib.rc('font', **font)
import matplotlib.pyplot as plt
```



```{.python}
# Create an instance of SVM and fit the data. 
C = 100.0  # SVM regularization parameter
gammas = [0.001, 0.01, 0.1, 1]


per_class=30
num_samps = 20
# Set-up 2x2 grid for plotting.
fig, ax = plt.subplots(1, 4, figsize=(10,3))
xlim=None
ylim=None
for samp in range(num_samps):
    X, y=create_data(per_class)
    models = []
    titles = []
    for gamma in gammas:
        models.append(svm.SVC(kernel='rbf', gamma=gamma, C=C))
        titles.append('$\gamma={}$'.format(gamma))
    models = (cl.fit(X, y) for cl in models)
    xlim, ylim = decision_boundary_plot(models, X, y, 
                           axs=ax, 
                           filename='../slides/diagrams/ml/bias-variance{samp:0>3}.svg'.format(samp=samp), 
                           titles=titles,
                          xlim=xlim,
                          ylim=ylim)
```










-->
<p>Helper function for sampling data from two different classes.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> numpy <span class="im">as</span> np</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> create_data(per_cluster<span class="op">=</span><span class="dv">30</span>):
    <span class="co">&quot;&quot;&quot;Create a randomly sampled data set</span>
<span class="co">    </span>
<span class="co">    :param per_cluster: number of points in each cluster</span>
<span class="co">    &quot;&quot;&quot;</span>
    X <span class="op">=</span> []
    y <span class="op">=</span> []
    scale <span class="op">=</span> <span class="dv">3</span>
    prec <span class="op">=</span> <span class="dv">1</span><span class="op">/</span>(scale<span class="op">*</span>scale)
    pos_mean <span class="op">=</span> [[<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>],[<span class="dv">0</span>,<span class="fl">0.5</span>],[<span class="dv">1</span>,<span class="dv">0</span>]]
    pos_cov <span class="op">=</span> [[prec, <span class="dv">0</span>.], [<span class="dv">0</span>., prec]]
    neg_mean <span class="op">=</span> [[<span class="dv">0</span>, <span class="op">-</span><span class="fl">0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>],[<span class="dv">0</span>,<span class="op">-</span><span class="fl">0.5</span>]]
    neg_cov <span class="op">=</span> [[prec, <span class="dv">0</span>.], [<span class="dv">0</span>., prec]]
    <span class="cf">for</span> mean <span class="kw">in</span> pos_mean:
        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>pos_cov, size<span class="op">=</span>per_class))
        y.append(np.ones((per_class, <span class="dv">1</span>)))
    <span class="cf">for</span> mean <span class="kw">in</span> neg_mean:
        X.append(np.random.multivariate_normal(mean<span class="op">=</span>mean, cov<span class="op">=</span>neg_cov, size<span class="op">=</span>per_class))
        y.append(np.zeros((per_class, <span class="dv">1</span>)))
    <span class="cf">return</span> np.vstack(X), np.vstack(y).flatten()</code></pre></div>
<p>Helper function for plotting the decision boundary of the SVM.</p>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> plot_contours(ax, cl, xx, yy, <span class="op">**</span>params):
    <span class="co">&quot;&quot;&quot;Plot the decision boundaries for a classifier.</span>

<span class="co">    :param ax: matplotlib axes object</span>
<span class="co">    :param cl: a classifier</span>
<span class="co">    :param xx: meshgrid ndarray</span>
<span class="co">    :param yy: meshgrid ndarray</span>
<span class="co">    :param params: dictionary of params to pass to contourf, optional</span>
<span class="co">    &quot;&quot;&quot;</span>
    Z <span class="op">=</span> cl.decision_function(np.c_[xx.ravel(), yy.ravel()])
    Z <span class="op">=</span> Z.reshape(xx.shape)
    <span class="co"># Plot decision boundary and regions</span>
    out <span class="op">=</span> ax.contour(xx, yy, Z, 
                     levels<span class="op">=</span>[<span class="op">-</span><span class="dv">1</span>., <span class="dv">0</span>., <span class="dv">1</span>], 
                     colors<span class="op">=</span><span class="st">&#39;black&#39;</span>, 
                     linestyles<span class="op">=</span>[<span class="st">&#39;dashed&#39;</span>, <span class="st">&#39;solid&#39;</span>, <span class="st">&#39;dashed&#39;</span>])
    out <span class="op">=</span> ax.contourf(xx, yy, Z, 
                     levels<span class="op">=</span>[Z.<span class="bu">min</span>(), <span class="dv">0</span>, Z.<span class="bu">max</span>()], 
                     colors<span class="op">=</span>[[<span class="fl">0.5</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>], [<span class="fl">1.0</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>]])
    <span class="cf">return</span> out</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> decision_boundary_plot(models, X, y, axs, filename, titles, xlim, ylim):
    <span class="co">&quot;&quot;&quot;Plot a decision boundary on the given axes</span>
<span class="co">    </span>
<span class="co">    :param axs: the axes to plot on.</span>
<span class="co">    :param models: the SVM models to plot</span>
<span class="co">    :param titles: the titles for each axis</span>
<span class="co">    :param X: input training data</span>
<span class="co">    :param y: target training data&quot;&quot;&quot;</span>
    <span class="cf">for</span> ax <span class="kw">in</span> axs.flatten():
        ax.clear()
    X0, X1 <span class="op">=</span> X[:, <span class="dv">0</span>], X[:, <span class="dv">1</span>]
    <span class="cf">if</span> xlim <span class="kw">is</span> <span class="va">None</span>:
        xlim <span class="op">=</span> [X0.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X0.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]
    <span class="cf">if</span> ylim <span class="kw">is</span> <span class="va">None</span>:
        ylim <span class="op">=</span> [X1.<span class="bu">min</span>()<span class="op">-</span><span class="dv">1</span>, X1.<span class="bu">max</span>()<span class="op">+</span><span class="dv">1</span>]
    xx, yy <span class="op">=</span> np.meshgrid(np.arange(xlim[<span class="dv">0</span>], xlim[<span class="dv">1</span>], <span class="fl">0.02</span>),
                         np.arange(ylim[<span class="dv">0</span>], ylim[<span class="dv">1</span>], <span class="fl">0.02</span>))
    <span class="cf">for</span> cl, title, ax <span class="kw">in</span> <span class="bu">zip</span>(models, titles, axs.flatten()):
        plot_contours(ax, cl, xx, yy,
                      cmap<span class="op">=</span>plt.cm.coolwarm, alpha<span class="op">=</span><span class="fl">0.8</span>)
        ax.plot(X0[y<span class="op">==</span><span class="dv">1</span>], X1[y<span class="op">==</span><span class="dv">1</span>], <span class="st">&#39;r.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)
        ax.plot(X0[y<span class="op">==</span><span class="dv">0</span>], X1[y<span class="op">==</span><span class="dv">0</span>], <span class="st">&#39;g.&#39;</span>, markersize<span class="op">=</span><span class="dv">10</span>)
        ax.set_xlim(xlim)
        ax.set_ylim(ylim)
        ax.set_xticks(())
        ax.set_yticks(())
        ax.set_title(title)
        mlai.write_figure(os.path.join(filename),
                          figure<span class="op">=</span>fig,
                          transparent<span class="op">=</span><span class="va">True</span>)
    <span class="cf">return</span> xlim, ylim</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="im">import</span> matplotlib
font <span class="op">=</span> {<span class="st">&#39;family&#39;</span> : <span class="st">&#39;sans&#39;</span>,
        <span class="st">&#39;weight&#39;</span> : <span class="st">&#39;bold&#39;</span>,
        <span class="st">&#39;size&#39;</span>   : <span class="dv">22</span>}

matplotlib.rc(<span class="st">&#39;font&#39;</span>, <span class="op">**</span>font)
<span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</code></pre></div>
<div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="co"># Create an instance of SVM and fit the data. </span>
C <span class="op">=</span> <span class="fl">100.0</span>  <span class="co"># SVM regularization parameter</span>
gammas <span class="op">=</span> [<span class="fl">0.001</span>, <span class="fl">0.01</span>, <span class="fl">0.1</span>, <span class="dv">1</span>]


per_class<span class="op">=</span><span class="dv">30</span>
num_samps <span class="op">=</span> <span class="dv">20</span>
<span class="co"># Set-up 2x2 grid for plotting.</span>
fig, ax <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">4</span>, figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">3</span>))
xlim<span class="op">=</span><span class="va">None</span>
ylim<span class="op">=</span><span class="va">None</span>
<span class="cf">for</span> samp <span class="kw">in</span> <span class="bu">range</span>(num_samps):
    X, y<span class="op">=</span>create_data(per_class)
    models <span class="op">=</span> []
    titles <span class="op">=</span> []
    <span class="cf">for</span> gamma <span class="kw">in</span> gammas:
        models.append(svm.SVC(kernel<span class="op">=</span><span class="st">&#39;rbf&#39;</span>, gamma<span class="op">=</span>gamma, C<span class="op">=</span>C))
        titles.append(<span class="st">&#39;$\gamma=</span><span class="sc">{}</span><span class="st">$&#39;</span>.<span class="bu">format</span>(gamma))
    models <span class="op">=</span> (cl.fit(X, y) <span class="cf">for</span> cl <span class="kw">in</span> models)
    xlim, ylim <span class="op">=</span> decision_boundary_plot(models, X, y, 
                           axs<span class="op">=</span>ax, 
                           filename<span class="op">=</span><span class="st">&#39;../slides/diagrams/ml/bias-variance</span><span class="sc">{samp:0&gt;3}</span><span class="st">.svg&#39;</span>.<span class="bu">format</span>(samp<span class="op">=</span>samp), 
                           titles<span class="op">=</span>titles,
                          xlim<span class="op">=</span>xlim,
                          ylim<span class="op">=</span>ylim)</code></pre></div>
<!--
















-->
<!--















-->
<!--  -->
<!--  -->


