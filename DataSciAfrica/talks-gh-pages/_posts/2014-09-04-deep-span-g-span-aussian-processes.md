---
abstract: In this talk we describe how deep neural networks can be modified to produce
  deep Gaussian process models. The framework of deep Gaussian processes allow for
  unsupervised learning, transfer learning, semi-supervised learning, multi-task learning
  and principled handling of different data types (count data, binary data, heavy
  tailed noise distributions). The main challenge is to solve these models efficiently
  for massive data sets. That challenge is in reach through a new class of variational
  approximations known as variational compression. The underlying variational bounds
  are very similar to the objective functions for deep neural networks, giving the
  promise of efficient approaches to deep learning that are constructed from components
  with very well understood analytical properties.
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Sheffield
  twitter: lawrennd
  url: http://inverseprobability.com
categories:
- Lawrence-ucl14b
day: '4'
errata: []
extras: []
key: Lawrence-ucl14b
layout: talk
linkpdf: http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/deepgp_ucl14.pdf
month: 9
published: 2014-09-04
section: pre
title: Deep <span>G</span>aussian Processes
venue: UCL-Duke University Workshop on Sensing and Analysis of High-Dimensional Data
videolectures: sahd2014_lawrence_gaussian_processes
year: '2014'
---