---
abstract: In this talk we describe how deep neural networks can be modified to produce
  deep Gaussian process models. The framework of deep Gaussian processes allow for
  unsupervised learning, transfer learning, semi-supervised learning, multi-task learning
  and principled handling of different data types (count data, binary data, heavy
  tailed noise distributions). The main challenge is to handle the intractabilities.
  In this talk we review the variational bounds that are used under the framework
  of variational compression and give some initial results of deep Gaussian process
  models.
author:
- family: Lawrence
  given: Neil D.
  gscholar: r3SJcvoAAAAJ
  institute: University of Sheffield
  twitter: lawrennd
  url: http://inverseprobability.com
categories:
- Lawrence-nyu15
day: '11'
errata: []
extras: []
key: Lawrence-nyu15
layout: talk
linkipynb: github.com/SheffieldML/deepGPy/blob/master/Nested%20Deep%20GPs.ipynb
linkpdf: http://staffwww.dcs.shef.ac.uk/people/N.Lawrence/talks/deepgp_nyu15.pdf
month: 5
published: 2015-05-11
section: pre
title: Deep <span>G</span>aussian Processes
venue: Computer Science Colloquium, NYU
year: '2015'
---