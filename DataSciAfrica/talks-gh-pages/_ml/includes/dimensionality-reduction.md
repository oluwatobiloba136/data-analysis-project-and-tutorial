### Dimensionality Reduction

* Compress the data by replacing the original data with reduced number of continuous variables.

\includesvg{../slides/diagrams/marionette.svg}

*We observe pose, $\dataVector$, puppeteer's hand, $\latentVector$ remains hidden to us.*

### Dimensionality Reduction

* Position of each body part of a marionette could be thought of as our data, $\inputVector_i$.

* Each data point is the 3-D co-ordinates of all the different body parts 

* Movement of parts determined by puppeteer via strings.

* For a simple puppet with one stick can move the stick up and down, left and right and twist.

### Dimensionality Reduction

* This gives three parameters in the puppeteers control.

* Implies that the puppet we see moving is controlled by only 3 variables.

* These 3 variables are often called the hidden or *latent variables*. 

* Assume similar for real world data, observations are derived from lower dimensional underlying process

### Examples in Social Sciences

* Underpins *psychological scoring* such as *IQ* or *personality tests*

* Myers-Briggs assumes personality is four dimensional.

* Political belief (left/right wing).

* Also language modelling has taken similar approaches: [word2vec](https://arxiv.org/abs/1301.3781) 

### Principal Component Analysis

* Principal component analysis (PCA) a linear dimensionality reduction technique

* In Hotelling's formulation of PCA: a assume $\inputVector$ is a linear weighted sum of the latent factors of interest.

* E.g. IQ test we would try and predict subject $i$'s answer to the $j$th question with the following function

$$
\dataScalar_{ij} = \mappingFunction_j(\latentScalar_i; \weightVector).
$$

$\latentScalar_i$ would be the IQ of subject $i$ and $\mappingFunction_j(\cdot)$ would function relating IQ and question answer.

### Hotelling's PCA

* Assume function is linear function. This idea is taken from a wider field known as *factor analysis*, so Hotelling described the challenge as

$$
\mappingFunction_j(\latentScalar_i; \weightVector) = \weightScalar_j \latentScalar_i
$$

* Answer to the $j$th question is predicted to be a scaling of the subject's IQ.

* Scale factor is given by $\weightScalar_j$.

### Higher Latent Dimensions

* For more latent dimensions matrix of scales, $\weightVector$

$$
\mappingFunction_j(\latentVector_i; \weightVector) = \weightScalar_{1j} \latentScalar_{1i} + \weightScalar_{2j} \latentScalar_{2i}
$$

*  $\latentScalar_{1i}$ might be extrovert/introvert and $\latentScalar_{2i}$ might rational/perceptual


### Parameters

* Parameters $\weightVector$ are known as the factor *loadings* in FA.

* In PCA they are known as the principal components.

* To fit the model need *loadings*, $\weightVector$, and latent variables, $\latentMatrix$.

* Can use least squares (leads to *matrix factorization* and recommender systems).

* Recommender systems most elements of $\inputVector_i$ are missing.

### Probability

* PCA and factor analysis the unknown latent factors are dealt with through a probability distribution.

* Assume these "unknowns" are  drawn from a zero mean, unit variance normal distribution.

* That implies a particular *probability* density for data (PDF).

* The PDF has parameters depending on factor loadings to be estimated.


### Maximum Likelihood

* Fit model by "maximising likelihood of data" under the PDF.

* Maxium likelihood for  PCA is the *eigenvalue decomposition* of the data covariance matrix.

* Algorithmically simple and convenient, but slow to compute for very large data sets with many features and many subjects.

### Principal Component Analysis

\includesvg{../slides/diagrams/demManifoldPrint_all_1_2.svg}

*Visualization of the first two principal components of an artificial data set. The data was generated by taking an image of a handwritten digit, 6, and rotating it 360 times, one degree each time. The first two principal components have been extracted in the diagram. The underlying circular shape is derived from the rotation of the data. Each image in the data set is projected on to the location its projected to in the latent space.*
